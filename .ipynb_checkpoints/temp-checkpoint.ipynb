{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wmmtaEz5ToLv"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "# from google.colab.patches import cv2_imshow\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.geometry import Point\n",
    "from shapely.ops import unary_union\n",
    "from numba import cuda, jit, njit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S1dniWFcTwEu"
   },
   "outputs": [],
   "source": [
    "#video from camera starts with 760 width 428 height\n",
    "def video_to_frame_arr(video_path):\n",
    "  cap = cv2.VideoCapture(video_path)\n",
    "  frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "  max_frame = frame_count\n",
    "  output_arr = []\n",
    "  # naive version (took 24s just appending frames)\n",
    "  success, img = cap.read()\n",
    "  while max_frame >= 0:\n",
    "    max_frame = max_frame - 1\n",
    "    output_arr.append(img)\n",
    "    # read next frame\n",
    "    success, img = cap.read()\n",
    "  return output_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fh6fAQ7oTxVl"
   },
   "outputs": [],
   "source": [
    "def get_keypoints_descriptors(image):\n",
    "  # Reading the image and converting into B/W\n",
    "  #image = cv2.imread(image_path)\n",
    "  gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Applying the function\n",
    "  sift = cv2.xfeatures2d.SIFT_create()\n",
    "  kp, des = sift.detectAndCompute(gray_image, None)\n",
    "    \n",
    "  # uncomment to draw keypoints on image\n",
    "  #kp_image = cv2.drawKeypoints(image, kp, None, color=(\n",
    "      #0, 255, 0)\n",
    "      #, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "  \n",
    "  return(kp, des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z1o5r-QqT3JA"
   },
   "outputs": [],
   "source": [
    "def feature_matching(des, des2):\n",
    "  FLANN_INDEX_KDTREE = 1\n",
    "  index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "  search_params = dict(checks = 50)\n",
    "  flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "  matches = flann.knnMatch(des,des2,k=2)\n",
    "  # store all the good matches as per Lowe's ratio test.\n",
    "  good = []\n",
    "  for m,n in matches:\n",
    "      if m.distance < 0.7*n.distance:\n",
    "          good.append(m)\n",
    "  return good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "-jwNck2tT3GM",
    "outputId": "b8616921-51fb-44f4-d2e8-e7da6ce14ca2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n  draw_params = dict(matchColor = (0,255,0), # draw matches in green \\n                    singlePointColor = None,\\n                    matchesMask = matchesMask, # draw only inliers\\n                    flags = cv2.DrawMatchesFlags_DEFAULT) #flags was 2\\n  image3 = cv2.drawMatches(image,kp,image2,kp2,good,None,**draw_params)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_homography_matrix(image, image2, kp, kp2, good, MIN_MATCH_COUNT):\n",
    "  if len(good)>MIN_MATCH_COUNT:\n",
    "      src_pts = np.float32([ kp[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "      dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "      M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "      matchesMask = mask.ravel().tolist()\n",
    "      h = image.shape[0]\n",
    "      w = image.shape[1]\n",
    "      pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "      dst = cv2.perspectiveTransform(pts,M)\n",
    "      #image2_lines = cv2.polylines(image2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "  else:\n",
    "      print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n",
    "      matchesMask = None\n",
    "      M = []\n",
    "  return M\n",
    "#uncomment this if you want to circle the RANSAC inliers/outliers and how they connect between the images\n",
    "\"\"\"\n",
    "  draw_params = dict(matchColor = (0,255,0), # draw matches in green \n",
    "                    singlePointColor = None,\n",
    "                    matchesMask = matchesMask, # draw only inliers\n",
    "                    flags = cv2.DrawMatchesFlags_DEFAULT) #flags was 2\n",
    "  image3 = cv2.drawMatches(image,kp,image2,kp2,good,None,**draw_params)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ltDLZAYIT3CD"
   },
   "outputs": [],
   "source": [
    "#this is the same as get_homography_matrix but it also returns a side by side of the two images with RANSAC inliers/outliers circled and the inliers connected between images\n",
    "def get_homography_matrix_old(image, image2, kp, kp2, good, MIN_MATCH_COUNT):\n",
    "  if len(good)>MIN_MATCH_COUNT:\n",
    "      src_pts = np.float32([ kp[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "      dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "      M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "      matchesMask = mask.ravel().tolist()\n",
    "      h = image.shape[0]\n",
    "      w = image.shape[1]\n",
    "      pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "      dst = cv2.perspectiveTransform(pts,M)\n",
    "      #image2_lines = cv2.polylines(image2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "  else:\n",
    "      print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n",
    "      matchesMask = None\n",
    "      M = []\n",
    "  draw_params = dict(matchColor = (0,255,0), # draw matches in green \n",
    "                    singlePointColor = None,\n",
    "                    matchesMask = matchesMask, # draw only inliers\n",
    "                    flags = cv2.DrawMatchesFlags_DEFAULT) #flags was 2\n",
    "  image3 = cv2.drawMatches(image,kp,image2,kp2,good,None,**draw_params)\n",
    "  return M, image3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "wosG7xiqT3AV"
   },
   "outputs": [],
   "source": [
    "def warpTwoImages(img1, img2, H):\n",
    "    '''warp img2 to img1 with homography matrix H'''\n",
    "    h1,w1 = img1.shape[:2]\n",
    "    h2,w2 = img2.shape[:2]\n",
    "    pts1 = np.float32([[0,0],[0,h1],[w1,h1],[w1,0]]).reshape(-1,1,2)\n",
    "    pts2 = np.float32([[0,0],[0,h2],[w2,h2],[w2,0]]).reshape(-1,1,2)\n",
    "    pts2_ = cv2.perspectiveTransform(pts2, H)\n",
    "    pts = np.concatenate((pts1, pts2_), axis=0)\n",
    "    [xmin, ymin] = np.int32(pts.min(axis=0).ravel() - 0.5)\n",
    "    [xmax, ymax] = np.int32(pts.max(axis=0).ravel() + 0.5)\n",
    "    t = [-xmin,-ymin]\n",
    "    Ht = np.array([[1,0,t[0]],[0,1,t[1]],[0,0,1]]) # translate\n",
    "\n",
    "    result = cv2.warpPerspective(img2, Ht.dot(H), (xmax-xmin, ymax-ymin))\n",
    "    result[t[1]:h1+t[1],t[0]:w1+t[0]] = img1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "GA9z5pinT29y"
   },
   "outputs": [],
   "source": [
    "#save array of frames to video \n",
    "def save_to_video(output_frame_arr, fps):\n",
    "  height,width,layers=output_frame_arr[0].shape\n",
    "  video=cv2.VideoWriter(filename = '/content/interval_five_later.avi',fourcc = 0,fps = fps,frameSize = (760, 428))\n",
    "  for j in range(len(output_frame_arr)):\n",
    "    video.write(output_frame_arr[j])\n",
    "  video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "gcFFlS-NT27l"
   },
   "outputs": [],
   "source": [
    "#resize all frames in an array to the same resolution (specify desired width and height as parameters)\n",
    "def resize_all(pano_frames_arr, width, height):\n",
    "  resized_pano_arr = []\n",
    "  height,width,layers=pano_frames_arr[0].shape\n",
    "  for i in range(len(pano_frames_arr)):\n",
    "    resized_pano_arr.append(cv2.resize(pano_frames_arr[i], (width, height)))\n",
    "  return resized_pano_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "IWHOjkgOT25g"
   },
   "outputs": [],
   "source": [
    "#given two frames, this function warps them to the same plane and translates them to their proper position on a shared background by adding padding where needed\n",
    "#You need to use this if you don't want the final panorama to be cropped to the resolution of one of the original frames\n",
    "def warpPerspectivePadded(src, dst, transf):\n",
    "\n",
    "    src_h, src_w = src.shape[:2]\n",
    "    lin_homg_pts = np.array([[0, src_w, src_w, 0], [0, 0, src_h, src_h], [1, 1, 1, 1]])\n",
    "\n",
    "    trans_lin_homg_pts = transf.dot(lin_homg_pts)\n",
    "    trans_lin_homg_pts /= trans_lin_homg_pts[2,:]\n",
    "\n",
    "    minX = np.min(trans_lin_homg_pts[0,:])\n",
    "    minY = np.min(trans_lin_homg_pts[1,:])\n",
    "    maxX = np.max(trans_lin_homg_pts[0,:])\n",
    "    maxY = np.max(trans_lin_homg_pts[1,:])\n",
    "\n",
    "    # calculate the needed padding and create a blank image to place dst within\n",
    "    dst_sz = list(dst.shape)\n",
    "    pad_sz = dst_sz.copy() # to get the same number of channels\n",
    "    pad_sz[0] = np.round(np.maximum(dst_sz[0], maxY) - np.minimum(0, minY)).astype(int)\n",
    "    pad_sz[1] = np.round(np.maximum(dst_sz[1], maxX) - np.minimum(0, minX)).astype(int)\n",
    "    dst_pad = np.zeros(pad_sz, dtype=np.uint8)\n",
    "\n",
    "    # add translation to the transformation matrix to shift to positive values\n",
    "    anchorX, anchorY = 0, 0\n",
    "    transl_transf = np.eye(3,3)\n",
    "    if minX < 0: \n",
    "        anchorX = np.round(-minX).astype(int)\n",
    "        transl_transf[0,2] += anchorX\n",
    "    if minY < 0:\n",
    "        anchorY = np.round(-minY).astype(int)\n",
    "        transl_transf[1,2] += anchorY\n",
    "    new_transf = transl_transf.dot(transf)\n",
    "    new_transf /= new_transf[2,2]\n",
    "    \n",
    "    dst_pad[anchorY:anchorY+dst_sz[0], anchorX:anchorX+dst_sz[1]] = dst\n",
    "    dest_pad_pre_warp = dst_pad\n",
    "\n",
    "    warped = cv2.warpPerspective(src, new_transf, (pad_sz[1],pad_sz[0]), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "\n",
    "    return dst_pad, warped, dest_pad_pre_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yl-B6skhT23M"
   },
   "outputs": [],
   "source": [
    "#warps a single point from one plane to another given a homography matrix M\n",
    "def warp_point(x, y, M):\n",
    "    d = M[2][0] * x + M[2][1] * y + M[2][2]\n",
    "\n",
    "    return (\n",
    "        int((M[0, 0] * x + M[0, 1] * y + M[0, 2]) / d), # x\n",
    "        int((M[1, 0] * x + M[1, 1] * y + M[1, 2]) / d), # y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "4ICFTjb7T20t"
   },
   "outputs": [],
   "source": [
    "#this is like warpPerspectivePadded but for N frames instead of 2\n",
    "def warp_n_with_padding(dst, src_list, transf_list, main_frame_arr):\n",
    "  #main_frame_arr = main_frame_arr2\n",
    "  #dst = main_frame_arr[505]\n",
    "  #src_list = [main_frame_arr[450], main_frame_arr[480], main_frame_arr[508], main_frame_arr[512], main_frame_arr[525]]\n",
    "  #transf_list = [hm13, hm23, hm43, hm53, hm63]\n",
    "\n",
    "  pad_sz_0_arr = []\n",
    "  pad_sz_1_arr = []\n",
    "  minMaxXY_arr = []\n",
    "  dst_sz = list(dst.shape)\n",
    "\n",
    "  for i in range(len(src_list)):\n",
    "    src_h, src_w = src_list[i].shape[:2]\n",
    "    lin_homg_pts = np.array([[0, src_w, src_w, 0], [0, 0, src_h, src_h], [1, 1, 1, 1]])\n",
    "    trans_lin_homg_pts = transf_list[i].dot(lin_homg_pts)\n",
    "    trans_lin_homg_pts /= trans_lin_homg_pts[2,:]\n",
    "\n",
    "    minX = np.min(trans_lin_homg_pts[0,:])\n",
    "    minY = np.min(trans_lin_homg_pts[1,:])\n",
    "    maxX = np.max(trans_lin_homg_pts[0,:])\n",
    "    maxY = np.max(trans_lin_homg_pts[1,:])\n",
    "\n",
    "    pad_sz0 = np.round(np.maximum(dst_sz[0], maxY) - np.minimum(0, minY)).astype(int)\n",
    "    pad_sz1 = np.round(np.maximum(dst_sz[1], maxX) - np.minimum(0, minX)).astype(int)\n",
    "\n",
    "    minMaxXY_arr.append([minX, minY, maxX, maxY])\n",
    "    pad_sz_0_arr.append(pad_sz0)\n",
    "    pad_sz_1_arr.append(pad_sz1)\n",
    "\n",
    "  # calculate the needed padding and create a blank image to place dst within\n",
    "  pad_sz = dst_sz.copy() # to get the same number of channels\n",
    "  pad_sz[0] = max(pad_sz_0_arr)\n",
    "  pad_sz[1] = max(pad_sz_1_arr)\n",
    "  indexY = pad_sz_0_arr.index(pad_sz[0])\n",
    "  indexX = pad_sz_1_arr.index(pad_sz[1])\n",
    "  minY = minMaxXY_arr[indexY][1]\n",
    "  maxY = minMaxXY_arr[indexY][3]\n",
    "  minX = minMaxXY_arr[indexX][0]\n",
    "  maxX = minMaxXY_arr[indexX][2]\n",
    "  dst_pad = np.zeros(pad_sz, dtype=np.uint8)\n",
    "\n",
    "  #add translation to ALL transformation matrices to shift to positive values\n",
    "  new_transf_list = []\n",
    "  anchorX_list = []\n",
    "  anchorY_list = []\n",
    "  for i in range(len(transf_list)):\n",
    "    anchorX, anchorY = 0, 0\n",
    "    transl_transf = np.eye(3,3)\n",
    "    if minX < 0: \n",
    "        anchorX = np.round(-minX).astype(int)\n",
    "        transl_transf[0,2] += anchorX\n",
    "    if minY < 0:\n",
    "        anchorY = np.round(-minY).astype(int)\n",
    "        transl_transf[1,2] += anchorY\n",
    "    new_transf = transl_transf.dot(transf_list[i])\n",
    "    new_transf /= new_transf[2,2]\n",
    "    new_transf_list.append(new_transf)\n",
    "    anchorX_list.append(anchorX)\n",
    "    anchorY_list.append(anchorY)\n",
    "\n",
    "  anchorX = max(anchorX_list)\n",
    "  anchorY = max(anchorY_list)\n",
    "  dst_pad[anchorY:anchorY+dst_sz[0], anchorX:anchorX+dst_sz[1]] = dst\n",
    "\n",
    "  warped_src_arr = []\n",
    "  for i in range(len(src_list)):\n",
    "    warped = cv2.warpPerspective(src_list[i], new_transf_list[i], (pad_sz[1],pad_sz[0]), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "    warped_src_arr.append(warped)\n",
    "  \n",
    "  return dst_pad, warped_src_arr, new_transf_list, anchorX, anchorY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "B-zSJ30PT2x7"
   },
   "outputs": [],
   "source": [
    "#converts all the warped + translated pieces of the panorama from RGB to RGBA images (you'll need the alpha channel [which sets opacity] for compositing)\n",
    "def get_rgba_im_arr(dst_pad, warped_src_arr):  \n",
    "  im_arr = []\n",
    "  im = Image.fromarray(dst_pad)\n",
    "  im = im.convert(\"RGBA\")\n",
    "  im = np.asarray(im)\n",
    "  im_arr.append(im)\n",
    "  for i in range(len(warped_src_arr)):\n",
    "    im2 = Image.fromarray(warped_src_arr[i])\n",
    "    im2 = im2.convert(\"RGBA\")\n",
    "    im2 = np.asarray(im2)\n",
    "    im_arr.append(im2)\n",
    "  \n",
    "  return im_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ic-lcmbBUgcH"
   },
   "outputs": [],
   "source": [
    "#converts an array containing an RGB image to an array containing an RGBA image (adds alpha channel)\n",
    "def rgba_to_rgb(comp_arr):\n",
    "  im = Image.fromarray((comp_arr).astype(np.uint8))\n",
    "  im = im.convert('RGB')\n",
    "  im = np.asarray(im)\n",
    "  return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3Y-iTlFhvzap"
   },
   "outputs": [],
   "source": [
    "#converts an array containing an RGB image to an array containing an RGBA image (adds alpha channel)\n",
    "def rgb_to_rgba(im):\n",
    "  im = Image.fromarray((im).astype(np.uint8))\n",
    "  im = im.convert('RGBA')\n",
    "  im = np.asarray(im)\n",
    "  return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WJ504hliUgZf",
    "outputId": "fb9643a3-aa0b-49b0-accc-a876551bc8ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n #if all images are black, set to black\\n      if len(not_black_list) == 0:\\n        comp_inner.append([0, 0, 0, 255])\\n      '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this function is like. the slowest possible way to do this. Will be updating soon\n",
    "def alpha_composite_n_images(im_arr):\n",
    "  #naive solution\n",
    "  comp = []\n",
    "  im = im_arr[0]\n",
    "  for row in range(im.shape[0]):\n",
    "    comp_inner = []\n",
    "    for col in range(im.shape[1]):\n",
    "      #figure out which images are black at this pixel\n",
    "      not_black_list = []\n",
    "      black_list = []\n",
    "      for i in range(len(im_arr)):\n",
    "        if im_arr[i][row][col][0] == 0 and im_arr[i][row][col][1] == 0 and im_arr[i][row][col][2] == 0:\n",
    "          black_list.append(im_arr[i])\n",
    "        else:\n",
    "          not_black_list.append(im_arr[i])\n",
    "      #if all images are black, set to transparent\n",
    "      if len(not_black_list) == 0:\n",
    "        comp_inner.append([0, 0, 0, 0])\n",
    "      #if only one image is NOT black, use it\n",
    "      elif len(not_black_list) == 1:\n",
    "        comp_inner.append(not_black_list[0][row][col])\n",
    "      #if multiple images are not black, alpha blend them all together\n",
    "      else:\n",
    "        alpha = 1/len(not_black_list)\n",
    "        channel1 = 0\n",
    "        channel2 = 0\n",
    "        channel3 = 0\n",
    "        for j in range(len(not_black_list)):\n",
    "            channel1 = channel1 + alpha * not_black_list[j][row][col][0]\n",
    "            channel2 = channel2 + alpha * not_black_list[j][row][col][1]\n",
    "            channel3 = channel3 + alpha * not_black_list[j][row][col][2]\n",
    "        comp_inner.append([channel1, channel2, channel3, 255])\n",
    "    comp.append(comp_inner)\n",
    "  comp_arr = np.array(comp)\n",
    "  return comp_arr\n",
    "\n",
    "\"\"\"\n",
    " #if all images are black, set to black\n",
    "      if len(not_black_list) == 0:\n",
    "        comp_inner.append([0, 0, 0, 255])\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "G43Q1AuntwSx"
   },
   "outputs": [],
   "source": [
    "\n",
    "@njit(parallel=True)\n",
    "def alpha_composite_n_images_parallel(im_arr):\n",
    "    im = im_arr[0]\n",
    "    comp = np.zeros((im.shape[0], im.shape[1], 4), dtype=np.float32)\n",
    "\n",
    "    for row in range(im.shape[0]):\n",
    "        for col in range(im.shape[1]):\n",
    "            #figure out which images are black at this pixel\n",
    "            not_black_list = []\n",
    "            black_list = []\n",
    "            for i in range(len(im_arr)):\n",
    "                if im_arr[i][row][col][0] == 0 and im_arr[i][row][col][1] == 0 and im_arr[i][row][col][2] == 0:\n",
    "                    black_list.append(im_arr[i])\n",
    "                else:\n",
    "                    not_black_list.append(im_arr[i])\n",
    "            #if all images are black, set to transparent\n",
    "            if len(not_black_list) == 0:\n",
    "                comp[row][col][0] = 0\n",
    "                comp[row][col][1] = 0\n",
    "                comp[row][col][2] = 0\n",
    "                comp[row][col][3] = 0\n",
    "            #if only one image is NOT black, use it\n",
    "            elif len(not_black_list) == 1:\n",
    "                comp[row][col] = not_black_list[0][row][col]\n",
    "            #if multiple images are not black, alpha blend them all together\n",
    "            else:\n",
    "                alpha = 1/len(not_black_list)\n",
    "                channel1 = 0\n",
    "                channel2 = 0\n",
    "                channel3 = 0\n",
    "                for j in range(len(not_black_list)):\n",
    "                    channel1 = channel1 + alpha * not_black_list[j][row][col][0]\n",
    "                    channel2 = channel2 + alpha * not_black_list[j][row][col][1]\n",
    "                    channel3 = channel3 + alpha * not_black_list[j][row][col][2]\n",
    "                comp[row][col][0] = channel1\n",
    "                comp[row][col][1] = channel2\n",
    "                comp[row][col][2] = channel3\n",
    "                comp[row][col][3] = 255\n",
    "    return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "yCkWj7FJUoss"
   },
   "outputs": [],
   "source": [
    "def alpha_composite_two(im, im2):\n",
    "  comp = []\n",
    "  #np.zeros((im.shape[0], im.shape[1], im.shape[2]))\n",
    "  alpha = 0.5\n",
    "  for row in range(im.shape[0]):\n",
    "    comp_inner = []\n",
    "    for col in range(im.shape[1]):\n",
    "      #if one image is black, just use the other\n",
    "      if (im[row][col][0] == 0 and im[row][col][1] == 0 and im[row][col][2] == 0) and (im2[row][col][0] != 0 or im2[row][col][1] != 0 or im2[row][col][2] != 0):\n",
    "        comp_inner.append(im2[row][col])\n",
    "      elif (im2[row][col][0] == 0 and im2[row][col][1] == 0 and im2[row][col][2] == 0) and (im[row][col][0] != 0 or im[row][col][1] != 0 or im[row][col][2] != 0):\n",
    "        comp_inner.append(im[row][col])\n",
    "      #if both images are black, set to transparent\n",
    "      elif (im[row][col][0] == 0 and im[row][col][1] == 0 and im[row][col][2] == 0) and (im2[row][col][0] == 0 and im2[row][col][1] == 0 and im2[row][col][2] == 0):\n",
    "        comp_inner.append([0, 0, 0, 0])\n",
    "      #if both pixels are not black, alpha blend\n",
    "      else:\n",
    "        channel1 = alpha * im[row][col][0] + (1 - alpha) * im2[row][col][0]\n",
    "        channel2 = alpha * im[row][col][1] + (1 - alpha) * im2[row][col][1]\n",
    "        channel3 = alpha * im[row][col][2] + (1 - alpha) * im2[row][col][2]\n",
    "        comp_inner.append([channel1, channel2, channel3, 255])\n",
    "    comp.append(comp_inner)\n",
    "  return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5f-4piGjkmRC"
   },
   "outputs": [],
   "source": [
    "#warps panorama back to rectangle after compositing. Doesn't work for every case yet\n",
    "#this version works by tracing where the corners of the original frames are warped to in the panorama and using those to \"pull\" it back into a rectangle\n",
    "def warp_back_to_rect_up(og_src, org_dst, final_width, final_height, anchorX, anchorY, hm_og_src_og_dst, comp_arr):\n",
    "  og_dst_width = org_dst.shape[1]\n",
    "  og_dst_height =org_dst.shape[0]\n",
    "  og_dst_corners = [[anchorX, anchorY], [anchorX + og_dst_width, anchorY], [anchorX + og_dst_width, anchorY + og_dst_height], [anchorX, anchorY + og_dst_height]] #clockwise from top left\n",
    "\n",
    "  og_src_width = og_src.shape[1]\n",
    "  og_src_height = og_src.shape[0]\n",
    "  og_src_warped_top_left = warp_point(0, 0, hm_og_src_og_dst)\n",
    "  og_src_warped_top_right = warp_point(og_src_width, 0, hm_og_src_og_dst)\n",
    "  og_src_warped_bottom_right = warp_point(og_src_width, og_src_height, hm_og_src_og_dst)\n",
    "  og_src_warped_bottom_left = warp_point(0, og_src_height, hm_og_src_og_dst)\n",
    "  og_src_warped_corners = [og_src_warped_top_left, og_src_warped_top_right, og_src_warped_bottom_right, og_src_warped_bottom_left]\n",
    "\n",
    "  src_quad_list = np.float32([og_dst_corners[0], og_dst_corners[1], og_src_warped_corners[2], og_src_warped_corners[3]])\n",
    "  dst_quad_list = np.float32([[0, 0], [final_width, 0], [final_width, final_height], [0, final_height]])\n",
    "\n",
    "  homography_matrix = cv2.getPerspectiveTransform(src_quad_list, dst_quad_list)\n",
    "\n",
    "  rect = cv2.warpPerspective(comp_arr, homography_matrix, (760, 428))\n",
    "\n",
    "  for j in range(rect.shape[0] - 1):\n",
    "    for i in range(len(rect[0])):\n",
    "      if rect[j][i][3] == 255:\n",
    "        top_left_x = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[0] - 1):\n",
    "    for i in range(len(rect[0])):\n",
    "      if rect[j][(len(rect[0]) - 1) - i][3] == 255:\n",
    "        top_right_x = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[0] - 1):\n",
    "    for i in range(rect.shape[1] - 1):\n",
    "      if rect[(rect.shape[0] - 1) - j][i][3] == 255:\n",
    "        bottom_left_x = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[0] - 1):\n",
    "    for i in range(len(rect[rect.shape[0] - 1])):\n",
    "      if rect[(rect.shape[0] - 1) - j][(rect.shape[0] - 1) - i][3] == 255:\n",
    "        bottom_right_x = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[1] - 1):\n",
    "    for i in range(len(rect[:,0])):\n",
    "      if rect[i][j][3] == 255:\n",
    "        top_left_y = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[1] - 1):\n",
    "    for i in range(rect.shape[0]):\n",
    "      if rect[i][(rect.shape[1] - 1) - j][3] == 255:\n",
    "        top_right_y = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[1] - 1):\n",
    "    for i in range(rect.shape[0]):\n",
    "      if rect[(rect.shape[0] - 1) - i][(rect.shape[1] - 1) - j][3] == 255:\n",
    "        bottom_right_y = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[1] - 1):\n",
    "    for i in range(len(rect[:,0])):\n",
    "      if rect[(rect.shape[0] - 1) - i][j][3] == 255:\n",
    "        bottom_left_y = i\n",
    "        break\n",
    "\n",
    "  #crop rectangle using where src_quad_list warped to as 4 corners \n",
    "  top_bound = max(top_left_y, top_right_y)\n",
    "  bottom_bound = rect.shape[0] - min(bottom_left_y, bottom_right_y)\n",
    "  left_bound = max(top_left_x, bottom_left_x)\n",
    "  right_bound = rect.shape[1] - min(top_right_x, bottom_right_x)\n",
    "\n",
    "  del_row_arr = []\n",
    "  #rows/y\n",
    "  for i in range(rect.shape[0]):\n",
    "    if i < top_bound or i > bottom_bound:\n",
    "      del_row_arr.append(i)\n",
    "  rect = np.delete(rect, del_row_arr, 0)\n",
    "\n",
    "  del_col_arr = []\n",
    "  #cols/x\n",
    "  for j in range(rect.shape[1]):\n",
    "    if j < left_bound or j > right_bound:\n",
    "      del_col_arr.append(j)\n",
    "  rect = np.delete(rect, del_col_arr, 1)\n",
    "\n",
    "  rect = cv2.resize(rect, (760, 428))\n",
    "\n",
    "  return rect, homography_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nFo5GIzdoSAu"
   },
   "outputs": [],
   "source": [
    "#crops any fully transparent rows or columns off an image\n",
    "def crop_transparent(comp_arr):\n",
    "  col_sums = np.sum(comp_arr, axis = 0)\n",
    "  del_cols_list = np.where(col_sums == 0)[0]\n",
    "  comp_arr = np.delete(comp_arr, del_cols_list, 1)\n",
    "  row_sums = np.sum(comp_arr, axis = 1)\n",
    "  del_rows_list = np.where(row_sums == 0)[0]\n",
    "  comp_arr = np.delete(comp_arr, del_rows_list, 0)\n",
    "  return comp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "frpbJzYOQZ1e"
   },
   "outputs": [],
   "source": [
    "#this is a helper function used for matching the video frames to object detection outputs\n",
    "# find element closest to given target using binary search.\n",
    " \n",
    "def findClosest(arr, n, target):\n",
    " \n",
    "    # Corner cases\n",
    "    if (target <= arr[0]):\n",
    "        return arr[0]\n",
    "    if (target >= arr[n - 1]):\n",
    "        return arr[n - 1]\n",
    " \n",
    "    # Doing binary search\n",
    "    i = 0; j = n; mid = 0\n",
    "    while (i < j):\n",
    "        mid = (i + j) // 2\n",
    " \n",
    "        if (arr[mid] == target):\n",
    "            return arr[mid]\n",
    " \n",
    "        # If target is less than array\n",
    "        # element, then search in left\n",
    "        if (target < arr[mid]) :\n",
    " \n",
    "            # If target is greater than previous\n",
    "            # to mid, return closest of two\n",
    "            if (mid > 0 and target > arr[mid - 1]):\n",
    "                return getClosest(arr[mid - 1], arr[mid], target)\n",
    " \n",
    "            # Repeat for left half\n",
    "            j = mid\n",
    "         \n",
    "        # If target is greater than mid\n",
    "        else :\n",
    "            if (mid < n - 1 and target < arr[mid + 1]):\n",
    "                return getClosest(arr[mid], arr[mid + 1], target)\n",
    "                 \n",
    "            # update i\n",
    "            i = mid + 1\n",
    "         \n",
    "    # Only single element left after search\n",
    "    return arr[mid]\n",
    " \n",
    " \n",
    "# Method to compare which one is the more close.\n",
    "# We find the closest by taking the difference\n",
    "# between the target and both values. It assumes\n",
    "# that val2 is greater than val1 and target lies\n",
    "# between these two.\n",
    "def getClosest(val1, val2, target):\n",
    " \n",
    "    if (target - val1 >= val2 - target):\n",
    "        return val2\n",
    "    else:\n",
    "        return val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "nHUTOaV9Qfs_"
   },
   "outputs": [],
   "source": [
    "def get_sec_from_start(current_timestamp, start_timestamp):\n",
    "  sec_from_start = (float(current_timestamp[0:-2]) - float(start_timestamp[0:-2]))/1000\n",
    "  return sec_from_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "OBkBSEuh75D1"
   },
   "outputs": [],
   "source": [
    "def warp_one_bbox(x, y, w, h, M):\n",
    "  \n",
    "  #warp top left corner \n",
    "  new_x, new_y = warp_point(x, y, M)\n",
    "\n",
    "  #warp bottom right corner\n",
    "  new_w, new_h = warp_point(w, h, M)\n",
    "\n",
    "  return new_x, new_y, new_w, new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ROdNi9aN-oG1"
   },
   "outputs": [],
   "source": [
    "#gets the x,y coords of how much the destination image was translated during the warping process\n",
    "def get_anchors(dst, src_list, transf_list, main_frame_arr):\n",
    "  pad_sz_0_arr = []\n",
    "  pad_sz_1_arr = []\n",
    "  minMaxXY_arr = []\n",
    "  dst_sz = list(dst.shape)\n",
    "\n",
    "  for i in range(len(src_list)):\n",
    "    src_h, src_w = src_list[i].shape[:2]\n",
    "    lin_homg_pts = np.array([[0, src_w, src_w, 0], [0, 0, src_h, src_h], [1, 1, 1, 1]])\n",
    "    trans_lin_homg_pts = transf_list[i].dot(lin_homg_pts)\n",
    "    trans_lin_homg_pts /= trans_lin_homg_pts[2,:]\n",
    "\n",
    "    minX = np.min(trans_lin_homg_pts[0,:])\n",
    "    minY = np.min(trans_lin_homg_pts[1,:])\n",
    "    maxX = np.max(trans_lin_homg_pts[0,:])\n",
    "    maxY = np.max(trans_lin_homg_pts[1,:])\n",
    "\n",
    "    pad_sz0 = np.round(np.maximum(dst_sz[0], maxY) - np.minimum(0, minY)).astype(int)\n",
    "    pad_sz1 = np.round(np.maximum(dst_sz[1], maxX) - np.minimum(0, minX)).astype(int)\n",
    "\n",
    "    minMaxXY_arr.append([minX, minY, maxX, maxY])\n",
    "    pad_sz_0_arr.append(pad_sz0)\n",
    "    pad_sz_1_arr.append(pad_sz1)\n",
    "\n",
    "  # calculate the needed padding and create a blank image to place dst within\n",
    "  pad_sz = dst_sz.copy() # to get the same number of channels\n",
    "  pad_sz[0] = max(pad_sz_0_arr)\n",
    "  pad_sz[1] = max(pad_sz_1_arr)\n",
    "  indexY = pad_sz_0_arr.index(pad_sz[0])\n",
    "  indexX = pad_sz_1_arr.index(pad_sz[1])\n",
    "  minY = minMaxXY_arr[indexY][1]\n",
    "  maxY = minMaxXY_arr[indexY][3]\n",
    "  minX = minMaxXY_arr[indexX][0]\n",
    "  maxX = minMaxXY_arr[indexX][2]\n",
    "  #dst_pad = np.zeros(pad_sz, dtype=np.uint8)\n",
    "\n",
    "  #add translation to ALL transformation matrices to shift to positive values\n",
    "  new_transf_list = []\n",
    "  anchorX_list = []\n",
    "  anchorY_list = []\n",
    "  for i in range(len(transf_list)):\n",
    "    anchorX, anchorY = 0, 0\n",
    "    transl_transf = np.eye(3,3)\n",
    "    if minX < 0: \n",
    "        anchorX = np.round(-minX).astype(int)\n",
    "        transl_transf[0,2] += anchorX\n",
    "    if minY < 0:\n",
    "        anchorY = np.round(-minY).astype(int)\n",
    "        transl_transf[1,2] += anchorY\n",
    "    new_transf = transl_transf.dot(transf_list[i])\n",
    "    new_transf /= new_transf[2,2]\n",
    "    new_transf_list.append(new_transf)\n",
    "    anchorX_list.append(anchorX)\n",
    "    anchorY_list.append(anchorY)\n",
    "\n",
    "  anchorX = max(anchorX_list)\n",
    "  anchorY = max(anchorY_list)\n",
    "  #dst_pad[anchorY:anchorY+dst_sz[0], anchorX:anchorX+dst_sz[1]] = dst\n",
    "\n",
    "  #warped_src_arr = []\n",
    "  #for i in range(len(src_list)):\n",
    "    #warped = cv2.warpPerspective(src_list[i], new_transf_list[i], (pad_sz[1],pad_sz[0]), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "    #warped_src_arr.append(warped)\n",
    "  \n",
    "  return anchorX, anchorY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4539/2507705882.py:5: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @jit()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#input: an array of frames and a list of indices to stitch (can be all)\n",
    "#outputs: an array of panoramas (on a fixed-size background), with each consecutive panorama adding the next frame from the input list of indices\n",
    "#this also writes all of the output frames to a folder so they can easily be saved/reloaded \n",
    "\n",
    "@jit()\n",
    "def stitch_one_at_a_time(indices, main_frame_arr, background_width, background_height): #I've been using background_width = 2280 and background_height = 1284 (3 times the width and height of one frame) as the default\n",
    "  final_pano_frames = []\n",
    "  output_min_x = 0\n",
    "  output_max_x = 0\n",
    "  output_min_y = 0\n",
    "  output_max_y = 0\n",
    "  \n",
    "  for f in range(1, len(indices)):\n",
    "  \n",
    "    if len(final_pano_frames) == 0: #there's some extra steps for the first one\n",
    "      src = rgb_to_rgba(main_frame_arr[indices[1]])\n",
    "      dst = rgb_to_rgba(main_frame_arr[indices[0]])\n",
    "\n",
    "      dst_sz = list(dst.shape)\n",
    "      pad_sz = [dst.shape[0] * 3, dst.shape[1] * 3, 4]\n",
    "      anchorX = dst.shape[1]-1 #this just places the first frame roughly in the center of the background (when the background is set to default values mentioned in above comment)\n",
    "      anchorY = dst.shape[0]-1\n",
    "\n",
    "      output_min_x = anchorX\n",
    "      output_max_x = anchorX + dst_sz[1]-1\n",
    "      output_min_y = anchorY\n",
    "      output_max_y = anchorY + dst_sz[0]-1\n",
    "\n",
    "      dst_pad = np.zeros(pad_sz, dtype=np.uint8)\n",
    "      dst_pad[anchorY:anchorY+dst_sz[0], anchorX:anchorX+dst_sz[1]] = dst\n",
    "\n",
    "      #warp src into correct place\n",
    "      kp_src, des_src = get_keypoints_descriptors(src)\n",
    "      kp_dst, des_dst = get_keypoints_descriptors(dst)\n",
    "      matches = feature_matching(des_src, des_dst)\n",
    "      transf = get_homography_matrix(src, dst, kp_src, kp_dst, matches, 4)\n",
    "\n",
    "      src_h, src_w = src.shape[:2]\n",
    "      lin_homg_pts = np.array([[0, src_w, src_w, 0], [0, 0, src_h, src_h], [1, 1, 1, 1]])\n",
    "\n",
    "      trans_lin_homg_pts = transf.dot(lin_homg_pts)\n",
    "      trans_lin_homg_pts /= trans_lin_homg_pts[2,:]\n",
    "\n",
    "      # add translation to the transformation matrix to shift to positive values\n",
    "      transl_transf = np.eye(3,3)\n",
    "      transl_transf[0,2] += anchorX\n",
    "      transl_transf[1,2] += anchorY\n",
    "      new_transf = transl_transf.dot(transf)\n",
    "      new_transf /= new_transf[2,2]\n",
    "\n",
    "      #take the corners of src and dst as polygons in shapely\n",
    "      pts = np.float32([ [0,0],[src_w, 0],[src_w, src_h],[0, src_h] ]).reshape(-1,1,2)\n",
    "      dst_pts = cv2.perspectiveTransform(pts,new_transf)\n",
    "\n",
    "      src_polygon = Polygon([(dst_pts[0][0][0], dst_pts[0][0][1]), (dst_pts[1][0][0], dst_pts[1][0][1]), (dst_pts[2][0][0], dst_pts[2][0][1]), (dst_pts[3][0][0], dst_pts[3][0][1])])\n",
    "      src_polygon_x,src_polygon_y = src_polygon.exterior.xy\n",
    "      dst_polygon = Polygon([(anchorX,anchorY),(anchorX+760,anchorY),(anchorX+760,anchorY+428),(anchorX,anchorY+428)])\n",
    "\n",
    "      #get overlap\n",
    "      overlap = src_polygon.intersection(dst_polygon)\n",
    "      overlap_x,overlap_y = overlap.exterior.xy\n",
    "      \n",
    "      #warp src\n",
    "      warped_src = cv2.warpPerspective(src, new_transf, (pad_sz[1],pad_sz[0]), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0, 0))\n",
    "\n",
    "      #use overlap_x, overlap_y to create \"bounding box\" around src polygon, jump to that bounding box and only check points inside of it for blending while compositing\n",
    "      #compositing dst_pad and warped_src\n",
    "      #keep dst_pad or warped_src values for pixels with only one or the other\n",
    "      #if a pixel is inside the overlap polygon, blend dst_pad and warped_src with equal weights\n",
    "      dst_pad_copy = dst_pad.copy()\n",
    "      min_x = math.floor(min(src_polygon_x))\n",
    "      max_x = math.floor(max(src_polygon_x))\n",
    "      min_y = math.floor(min(src_polygon_y))\n",
    "      max_y = math.floor(max(src_polygon_y))\n",
    "\n",
    "      output_min_x = min(output_min_x, min_x)\n",
    "      output_max_x = max(output_max_x, max_x)\n",
    "      output_min_y = min(output_min_y, min_y)\n",
    "      output_max_y = max(output_max_y, max_y)\n",
    "\n",
    "      for i in range(max_y - min_y):\n",
    "        for j in range(max_x - min_x):\n",
    "          current_point = Point(min_x + j, min_y + i)\n",
    "          if overlap.contains(current_point):\n",
    "            dst_pad_copy[min_y + i][min_x + j] = [(dst_pad_copy[min_y + i][min_x + j][0] / 2 + warped_src[min_y + i][min_x + j][0] / 2), (dst_pad_copy[min_y + i][min_x + j][1] / 2 + warped_src[min_y + i][min_x + j][1] / 2), (dst_pad_copy[min_y + i][min_x + j][2] / 2 + warped_src[min_y + i][min_x + j][2] / 2), 255]\n",
    "          elif src_polygon.contains(current_point):\n",
    "            dst_pad_copy[min_y + i][min_x + j] = warped_src[min_y + i][min_x + j]\n",
    "      \n",
    "      #save frames\n",
    "      final_pano_frames.append(dst_pad) #\"pano\" with only first frame/dst\n",
    "      final_pano_frames.append(dst_pad_copy) #pano with first two frames (src and dst)\n",
    "      cv2.imwrite(\"/content/final_pano_frames/\" + str(0) + \".png\", final_pano_frames[0][anchorY:anchorY+dst_sz[0]:1, anchorX:anchorX+dst_sz[1]:1])\n",
    "      cv2.imwrite(\"/content/final_pano_frames/\" + str(1) + \".png\", final_pano_frames[1][output_min_y:output_max_y+1:1, output_min_x:output_max_x+1:1])\n",
    "\n",
    "\n",
    "    else:\n",
    "      prev_dst_pad_copy = dst_pad_copy.copy()\n",
    "      src = rgb_to_rgba(main_frame_arr[indices[f]])\n",
    "      dst = prev_dst_pad_copy\n",
    "\n",
    "      #warp src into correct place\n",
    "      kp_src, des_src = get_keypoints_descriptors(src)\n",
    "      kp_dst, des_dst = get_keypoints_descriptors(dst)\n",
    "      matches = feature_matching(des_src, des_dst)\n",
    "      transf = get_homography_matrix(src, dst, kp_src, kp_dst, matches, 4)\n",
    "\n",
    "      #take the corners of src and dst as polygons in shapely\n",
    "      src_h, src_w = src.shape[:2]\n",
    "      pts = np.float32([ [0,0],[src_w, 0],[src_w, src_h],[0, src_h] ]).reshape(-1,1,2)\n",
    "      dst_pts = cv2.perspectiveTransform(pts,transf)\n",
    "\n",
    "      polygons = [src_polygon, dst_polygon]\n",
    "      dst_polygon = unary_union(polygons)\n",
    "\n",
    "      src_polygon = Polygon([(dst_pts[0][0][0], dst_pts[0][0][1]), (dst_pts[1][0][0], dst_pts[1][0][1]), (dst_pts[2][0][0], dst_pts[2][0][1]), (dst_pts[3][0][0], dst_pts[3][0][1])])\n",
    "      src_polygon_x,src_polygon_y = src_polygon.exterior.xy\n",
    "\n",
    "      #get overlap\n",
    "      overlap = src_polygon.intersection(dst_polygon)\n",
    "      overlap_x,overlap_y = overlap.exterior.xy\n",
    "\n",
    "      #warp src\n",
    "      warped_src = cv2.warpPerspective(src, transf, (prev_dst_pad_copy.shape[1],prev_dst_pad_copy.shape[0]), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0, 0))\n",
    "\n",
    "      #compositing\n",
    "      min_x = math.floor(min(src_polygon_x))\n",
    "      max_x = math.floor(max(src_polygon_x))\n",
    "      min_y = math.floor(min(src_polygon_y))\n",
    "      max_y = math.floor(max(src_polygon_y))\n",
    "\n",
    "      output_min_x = min(output_min_x, min_x)\n",
    "      output_max_x = max(output_max_x, max_x)\n",
    "      output_min_y = min(output_min_y, min_y)\n",
    "      output_max_y = max(output_max_y, max_y)\n",
    "\n",
    "      for i in range(max_y - min_y):\n",
    "        for j in range(max_x - min_x):\n",
    "          current_point = Point(min_x + j, min_y + i)\n",
    "          if overlap.contains(current_point):\n",
    "            prev_dst_pad_copy[min_y + i][min_x + j] = [(prev_dst_pad_copy[min_y + i][min_x + j][0] / 2 + warped_src[min_y + i][min_x + j][0] / 2), (prev_dst_pad_copy[min_y + i][min_x + j][1] / 2 + warped_src[min_y + i][min_x + j][1] / 2), (prev_dst_pad_copy[min_y + i][min_x + j][2] / 2 + warped_src[min_y + i][min_x + j][2] / 2), 255]\n",
    "          elif src_polygon.contains(current_point):\n",
    "            prev_dst_pad_copy[min_y + i][min_x + j] = warped_src[min_y + i][min_x + j]\n",
    "      \n",
    "      #save\n",
    "      final_pano_frames.append(prev_dst_pad_copy)\n",
    "      cv2.imwrite(\"/content/final_pano_frames/\" + str(f) + \".png\", final_pano_frames[f][output_min_y:output_max_y+1:1, output_min_x:output_max_x+1:1])\n",
    "\n",
    "  return final_pano_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Bounding Box / time\n",
    "- at every time step, draw all bounding boxes from the beginning of the vis to now\n",
    "- rgba : a inversely proportional to (distance from the present)^2\n",
    "-- play around with color, thickness & how much of the box is being drawn (whole box vs. picture-frame vs. just corners)\n",
    "\"\"\"\n",
    "\n",
    "def box_color(time_color, time_from_present, max_time=5):  # assumed seconds\n",
    "    \"\"\" \n",
    "    input: box's time from the present moment, how far in the past boxes should show up\n",
    "    output: [r, g, b, a] value of the box [0,255]\n",
    "    \"\"\"\n",
    "    if time_from_present >= max_time:\n",
    "        return [0,0,0,0]\n",
    "    color_factor = (1 - time_from_present/max_time)**2  # get the squared dist from max time by normalizing [0,1]\n",
    "    hex_color = time_color[round(color_factor * (len(time_color)-1))]\n",
    "    return rgb_to_bgr(hex_to_rgb(hex_color)) + [round(color_factor*255)]  # add alpha scaler to return\n",
    "\n",
    "def hex_to_rgb(hex_str):\n",
    "    \"\"\" helper to convert rgb hex to its individual values in a list \n",
    "    ref: https://stackoverflow.com/questions/29643352/converting-hex-to-rgb-value-in-python \"\"\"\n",
    "    h = hex_str.lstrip('#')\n",
    "    return list(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "def rgb_to_bgr(lst):  # pano works in bgr for some reason\n",
    "    return [lst[2], lst[1], lst[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Resources:\n",
    "- highlighting an area - https://stackoverflow.com/questions/56472024/how-to-change-the-opacity-of-boxes-cv2-rectangle\n",
    "- cv2 draw functions - https://docs.opencv.org/4.x/dc/da5/tutorial_py_drawing_functions.html\n",
    "\"\"\"\n",
    "\n",
    "def draw_side_box(image, x,y, w,h, color, thickness):\n",
    "    line_len1 = (w-x)//8\n",
    "    image = cv2.line(image, (x,y), (x+line_len1, y), color, thickness)\n",
    "    image = cv2.line(image, (x,y), (x,h), color, thickness)\n",
    "    image = cv2.line(image, (x,h), (x+line_len1, h), color, thickness) \n",
    "    return cv2.line(image, (x,(y+h)//2), (x,-line_len1+(y+h)//2), color, thickness)\n",
    "\n",
    "def draw_center_point(image, x,y, w,h, color, thickness):\n",
    "    rect_size =  3  # size of boxes that denote corners\n",
    "    newx = (x+w)//2\n",
    "    newy = (y+h)//2\n",
    "    return cv2.rectangle(image, (newx,newy), (newx+rect_size, newy+rect_size), color, rect_size)\n",
    "\n",
    "def draw_box_corners(image, x,y, w,h, color, thickness):\n",
    "    rect_size =  2  # size of boxes that denote corners\n",
    "    image = cv2.rectangle(image, (x,y), (x+rect_size, y+rect_size), color, rect_size)\n",
    "    image = cv2.rectangle(image, (w,y), (w+rect_size, y+rect_size), color, rect_size)\n",
    "    image = cv2.rectangle(image, (x,h), (x+rect_size, h+rect_size), color, rect_size)\n",
    "    return cv2.rectangle(image, (w,h), (w+rect_size, h+rect_size), color, rect_size)\n",
    "\n",
    "\n",
    "# line_len = 15;\n",
    "def draw_box_frame(image, x,y, w,h, color, thickness):\n",
    "    line_len1 = (w-x)//8\n",
    "    line_len2 = (y-h)//8\n",
    "    # TODO: tweak positioning\n",
    "    image = cv2.line(image, (x,y), (x+line_len1, y), color, thickness)\n",
    "    image = cv2.line(image, (x,y), (x, y-line_len2), color, thickness)\n",
    "    \n",
    "    image = cv2.line(image, (w-line_len1,y), (w,y), color, thickness)\n",
    "    image = cv2.line(image, (w,y), (w,y-line_len2), color, thickness)\n",
    "    \n",
    "    image = cv2.line(image, (x,h), (x+line_len1, h), color, thickness)\n",
    "    image = cv2.line(image, (x,h+line_len2), (x, h), color, thickness)\n",
    "    \n",
    "    image = cv2.line(image, (w-line_len1,h), (w, h), color, thickness)\n",
    "    return cv2.line(image, (w,h+line_len2), (w, h), color, thickness)\n",
    "    \n",
    "    \n",
    "def draw_box(image, x,y, w,h, color, thickness):\n",
    "    return cv2.rectangle(image, (x,y), (w,h), color, thickness)\n",
    "\n",
    "def draw_line(image, x,y, w,h, color, thickness):\n",
    "    # basic func to draw line from (x,y) to (w,h)\n",
    "    return cv2.line(image, (x,y), (w,h), color, thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "INPUTS:\n",
    "index_list: array of indices of the frames in main_frame_arr frames used to create panorama_image [NOTE: this function will assume all frames were warped to the plane of the frame at the first index in index_list (aka the first one is dst)]\n",
    "frames_timestamps_arr: array of timestamps corresponding to each frame used to create panorama_image\n",
    "detic_dict: object detection model outputs\n",
    "panorama_image: the panorama you want to draw bounding boxes on\n",
    "new_transf_list: an array of the homography matrices used to create panorama_image (should be returned by warp_n_with_padding)\n",
    "anchorX: the X translation used to create panorama_image (should be returned by warp_n_with_padding)\n",
    "anchorY: the Y translation used to create panorama_image (should be returned by warp_n_with_padding)\n",
    "colors_list: array of the bounding box colors for each timestep\n",
    "thickness: thickness of bounding boxes (2 seems to be a reasonable default)\n",
    "\n",
    "OUTPUTS:\n",
    "image with bounding boxes for all objects at from given indices \n",
    "\n",
    "\"\"\"\n",
    "def draw_all_bounding_boxes_for_given_indices(index_list, frames_timestamps_arr, detic_dict, panorama_image,\n",
    "                                              new_transf_list, anchorX, anchorY, colors_list, thickness, box_type='center_dot', object_subset={}):\n",
    "    image = panorama_image.copy() #so it doesn't draw directly on the panorama in case the one without bounding boxes is needed later\n",
    "    last_dot = {}\n",
    "    for f in range(len(index_list)):\n",
    "        index = next((i for i, obj in enumerate(detic_dict) if obj['timestamp'] == frames_timestamps_arr[index_list[f]]), -1)\n",
    "        for i in range(len(detic_dict[index][\"values\"])):\n",
    "            obj_name = detic_dict[index][\"values\"][i][\"class_id\"]\n",
    "            if len(object_subset)>0 and obj_name not in object_subset:\n",
    "                continue\n",
    "            x = int(detic_dict[index][\"values\"][i][\"xyxyn\"][0] * 760)\n",
    "            w = int(detic_dict[index][\"values\"][i][\"xyxyn\"][2] * 760)\n",
    "            y = int(detic_dict[index][\"values\"][i][\"xyxyn\"][1] * 428)\n",
    "            h = int(detic_dict[index][\"values\"][i][\"xyxyn\"][3] * 428)\n",
    "\n",
    "            if f == 0:  # first timestamp --> coord plane\n",
    "                x = x + anchorX\n",
    "                w = w + anchorX\n",
    "                y = y + anchorY\n",
    "                h = h + anchorY\n",
    "            else:\n",
    "                x = warp_point(x, y, new_transf_list[f-1])[0]\n",
    "                y = warp_point(x, y, new_transf_list[f-1])[1]\n",
    "                w = warp_point(w, h, new_transf_list[f-1])[0]\n",
    "                h = warp_point(w, h, new_transf_list[f-1])[1]\n",
    "\n",
    "            start_point = (x, y)\n",
    "            end_point = (w, h)\n",
    "\n",
    "            #draw object bounding box (draw_box, draw_box_frame, draw_box_corners)\n",
    "            new_color = box_color(colors_list, f, len(index_list))\n",
    "            \n",
    "            if box_type == 'box':\n",
    "                image = draw_box(image, x,y, w,h, new_color, thickness)\n",
    "            elif box_type == 'frame':\n",
    "                image = draw_box_frame(image, x,y, w,h, new_color, thickness)\n",
    "            elif box_type == 'corner_dot':\n",
    "                image = draw_box_corners(image, x,y, w,h, new_color, thickness)\n",
    "            elif box_type == 'center_dot':\n",
    "                image = draw_center_point(image, x,y, w,h, new_color, thickness)\n",
    "            elif box_type == 'center_dot_lined':\n",
    "                image = draw_center_point(image, x,y, w,h, new_color, thickness)\n",
    "                if obj_name in last_dot:\n",
    "                    xnew = last_dot[obj_name][0]\n",
    "                    ynew = last_dot[obj_name][1]\n",
    "                    image = draw_line(image, (x+w)//2, (y+h)//2, xnew,ynew, new_color, thickness//2)\n",
    "                last_dot[obj_name] = [(x+w)//2, (y+h)//2]\n",
    "            elif box_type == 'side_box':\n",
    "                image = draw_side_box(image, x,y, w,h, new_color, thickness)\n",
    "            else:\n",
    "                assert False\n",
    "            \n",
    "\n",
    "            text_color = [val for val in list(new_color)]  # saturated color version (255,255,255, 255)\n",
    "            text_color[-1] /= 5 # reduce alpha\n",
    "#             Draw background rectangle (text)\n",
    "            if box_type == 'box':\n",
    "                image = cv2.rectangle(image, (x, y-15), (x + (w - x), y), new_color, -1)\n",
    "\n",
    "            # Add text\n",
    "            text_color[-1] *= 5\n",
    "            if box_type == 'center_dot':\n",
    "                new_text = detic_dict[index][\"values\"][i][\"label\"]\n",
    "                image = cv2.putText(image, new_text, (int((x+w)/2-len(new_text)*2.5),(y+h)//2 - 8), cv2.FONT_HERSHEY_SIMPLEX, 0.3, text_color, 1)\n",
    "            elif box_type == 'box':\n",
    "                image = cv2.putText(image, detic_dict[index][\"values\"][i][\"label\"], (x + 2,y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255, 255), 1)\n",
    "            elif box_type == 'side_box':\n",
    "                image = cv2.putText(image, new_text, (x-5-len(new_text)*2.5,(y+h)//2), cv2.FONT_HERSHEY_SIMPLEX, 0.3, text_color, 1)\n",
    "            else:\n",
    "                image = cv2.putText(image, detic_dict[index][\"values\"][i][\"label\"], (x + 2,y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, text_color, 1)\n",
    "    return image"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
