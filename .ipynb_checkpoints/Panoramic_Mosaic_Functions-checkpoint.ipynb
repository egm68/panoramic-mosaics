{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wmmtaEz5ToLv"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S1dniWFcTwEu"
   },
   "outputs": [],
   "source": [
    "#video from camera starts with 760 width 428 height\n",
    "def video_to_frame_arr(video_path):\n",
    "  cap = cv2.VideoCapture(video_path)\n",
    "  frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "  max_frame = frame_count\n",
    "  output_arr = []\n",
    "  # naive version (took 24s just appending frames)\n",
    "  success, img = cap.read()\n",
    "  while max_frame >= 0:\n",
    "    max_frame = max_frame - 1\n",
    "    output_arr.append(img)\n",
    "    # read next frame\n",
    "    success, img = cap.read()\n",
    "  return output_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fh6fAQ7oTxVl"
   },
   "outputs": [],
   "source": [
    "def get_keypoints_descriptors(image):\n",
    "  # Reading the image and converting into B/W\n",
    "  #image = cv2.imread(image_path)\n",
    "  gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Applying the function\n",
    "  sift = cv2.xfeatures2d.SIFT_create()\n",
    "  kp, des = sift.detectAndCompute(gray_image, None)\n",
    "    \n",
    "  # uncomment to draw keypoints on image\n",
    "  #kp_image = cv2.drawKeypoints(image, kp, None, color=(\n",
    "      #0, 255, 0)\n",
    "      #, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "  \n",
    "  return(kp, des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z1o5r-QqT3JA"
   },
   "outputs": [],
   "source": [
    "def feature_matching(des, des2):\n",
    "  FLANN_INDEX_KDTREE = 1\n",
    "  index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "  search_params = dict(checks = 50)\n",
    "  flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "  matches = flann.knnMatch(des,des2,k=2)\n",
    "  # store all the good matches as per Lowe's ratio test.\n",
    "  good = []\n",
    "  for m,n in matches:\n",
    "      if m.distance < 0.7*n.distance:\n",
    "          good.append(m)\n",
    "  return good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "-jwNck2tT3GM",
    "outputId": "09ec9cf4-a04f-4db4-ef69-20100c395471"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n  draw_params = dict(matchColor = (0,255,0), # draw matches in green \\n                    singlePointColor = None,\\n                    matchesMask = matchesMask, # draw only inliers\\n                    flags = cv2.DrawMatchesFlags_DEFAULT) #flags was 2\\n  image3 = cv2.drawMatches(image,kp,image2,kp2,good,None,**draw_params)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_homography_matrix(image, image2, kp, kp2, good, MIN_MATCH_COUNT):\n",
    "  if len(good)>MIN_MATCH_COUNT:\n",
    "      src_pts = np.float32([ kp[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "      dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "      M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "      matchesMask = mask.ravel().tolist()\n",
    "      h = image.shape[0]\n",
    "      w = image.shape[1]\n",
    "      pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "      dst = cv2.perspectiveTransform(pts,M)\n",
    "      #image2_lines = cv2.polylines(image2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "  else:\n",
    "      print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n",
    "      matchesMask = None\n",
    "      M = []\n",
    "  return M\n",
    "#uncomment this if you want to circle the RANSAC inliers/outliers and how they connect between the images\n",
    "\"\"\"\n",
    "  draw_params = dict(matchColor = (0,255,0), # draw matches in green \n",
    "                    singlePointColor = None,\n",
    "                    matchesMask = matchesMask, # draw only inliers\n",
    "                    flags = cv2.DrawMatchesFlags_DEFAULT) #flags was 2\n",
    "  image3 = cv2.drawMatches(image,kp,image2,kp2,good,None,**draw_params)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ltDLZAYIT3CD"
   },
   "outputs": [],
   "source": [
    "#this is the same as get_homography_matrix but it also returns a side by side of the two images with RANSAC inliers/outliers circled and the inliers connected between images\n",
    "def get_homography_matrix_old(image, image2, kp, kp2, good, MIN_MATCH_COUNT):\n",
    "  if len(good)>MIN_MATCH_COUNT:\n",
    "      src_pts = np.float32([ kp[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "      dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n",
    "      M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "      matchesMask = mask.ravel().tolist()\n",
    "      h = image.shape[0]\n",
    "      w = image.shape[1]\n",
    "      pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n",
    "      dst = cv2.perspectiveTransform(pts,M)\n",
    "      #image2_lines = cv2.polylines(image2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "  else:\n",
    "      print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n",
    "      matchesMask = None\n",
    "      M = []\n",
    "  draw_params = dict(matchColor = (0,255,0), # draw matches in green \n",
    "                    singlePointColor = None,\n",
    "                    matchesMask = matchesMask, # draw only inliers\n",
    "                    flags = cv2.DrawMatchesFlags_DEFAULT) #flags was 2\n",
    "  image3 = cv2.drawMatches(image,kp,image2,kp2,good,None,**draw_params)\n",
    "  return M, image3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wosG7xiqT3AV"
   },
   "outputs": [],
   "source": [
    "def warpTwoImages(img1, img2, H):\n",
    "    '''warp img2 to img1 with homography matrix H'''\n",
    "    h1,w1 = img1.shape[:2]\n",
    "    h2,w2 = img2.shape[:2]\n",
    "    pts1 = np.float32([[0,0],[0,h1],[w1,h1],[w1,0]]).reshape(-1,1,2)\n",
    "    pts2 = np.float32([[0,0],[0,h2],[w2,h2],[w2,0]]).reshape(-1,1,2)\n",
    "    pts2_ = cv2.perspectiveTransform(pts2, H)\n",
    "    pts = np.concatenate((pts1, pts2_), axis=0)\n",
    "    [xmin, ymin] = np.int32(pts.min(axis=0).ravel() - 0.5)\n",
    "    [xmax, ymax] = np.int32(pts.max(axis=0).ravel() + 0.5)\n",
    "    t = [-xmin,-ymin]\n",
    "    Ht = np.array([[1,0,t[0]],[0,1,t[1]],[0,0,1]]) # translate\n",
    "\n",
    "    result = cv2.warpPerspective(img2, Ht.dot(H), (xmax-xmin, ymax-ymin))\n",
    "    result[t[1]:h1+t[1],t[0]:w1+t[0]] = img1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GA9z5pinT29y"
   },
   "outputs": [],
   "source": [
    "#save array of frames to video \n",
    "def save_to_video(output_frame_arr, fps):\n",
    "  height,width,layers=output_frame_arr[0].shape\n",
    "  video=cv2.VideoWriter(filename = '/content/interval_five_later.avi',fourcc = 0,fps = fps,frameSize = (760, 428))\n",
    "  for j in range(len(output_frame_arr)):\n",
    "    video.write(output_frame_arr[j])\n",
    "  video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gcFFlS-NT27l"
   },
   "outputs": [],
   "source": [
    "#resize all frames in an array to the same resolution (specify desired width and height as parameters)\n",
    "def resize_all(pano_frames_arr, width, height):\n",
    "  resized_pano_arr = []\n",
    "  height,width,layers=pano_frames_arr[0].shape\n",
    "  for i in range(len(pano_frames_arr)):\n",
    "    resized_pano_arr.append(cv2.resize(pano_frames_arr[i], (width, height)))\n",
    "  return resized_pano_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IWHOjkgOT25g"
   },
   "outputs": [],
   "source": [
    "#given two frames, this function warps them to the same plane and translates them to their proper position on a shared background by adding padding where needed\n",
    "#You need to use this if you don't want the final panorama to be cropped to the resolution of one of the original frames\n",
    "def warpPerspectivePadded(src, dst, transf):\n",
    "\n",
    "    src_h, src_w = src.shape[:2]\n",
    "    lin_homg_pts = np.array([[0, src_w, src_w, 0], [0, 0, src_h, src_h], [1, 1, 1, 1]])\n",
    "\n",
    "    trans_lin_homg_pts = transf.dot(lin_homg_pts)\n",
    "    trans_lin_homg_pts /= trans_lin_homg_pts[2,:]\n",
    "\n",
    "    minX = np.min(trans_lin_homg_pts[0,:])\n",
    "    minY = np.min(trans_lin_homg_pts[1,:])\n",
    "    maxX = np.max(trans_lin_homg_pts[0,:])\n",
    "    maxY = np.max(trans_lin_homg_pts[1,:])\n",
    "\n",
    "    # calculate the needed padding and create a blank image to place dst within\n",
    "    dst_sz = list(dst.shape)\n",
    "    pad_sz = dst_sz.copy() # to get the same number of channels\n",
    "    pad_sz[0] = np.round(np.maximum(dst_sz[0], maxY) - np.minimum(0, minY)).astype(int)\n",
    "    pad_sz[1] = np.round(np.maximum(dst_sz[1], maxX) - np.minimum(0, minX)).astype(int)\n",
    "    dst_pad = np.zeros(pad_sz, dtype=np.uint8)\n",
    "\n",
    "    # add translation to the transformation matrix to shift to positive values\n",
    "    anchorX, anchorY = 0, 0\n",
    "    transl_transf = np.eye(3,3)\n",
    "    if minX < 0: \n",
    "        anchorX = np.round(-minX).astype(int)\n",
    "        transl_transf[0,2] += anchorX\n",
    "    if minY < 0:\n",
    "        anchorY = np.round(-minY).astype(int)\n",
    "        transl_transf[1,2] += anchorY\n",
    "    new_transf = transl_transf.dot(transf)\n",
    "    new_transf /= new_transf[2,2]\n",
    "    print(new_transf)\n",
    "\n",
    "    dst_pad[anchorY:anchorY+dst_sz[0], anchorX:anchorX+dst_sz[1]] = dst\n",
    "    dest_pad_pre_warp = dst_pad\n",
    "\n",
    "    warped = cv2.warpPerspective(src, new_transf, (pad_sz[1],pad_sz[0]), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "\n",
    "    return dst_pad, warped, dest_pad_pre_warp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "yl-B6skhT23M"
   },
   "outputs": [],
   "source": [
    "#warps a single point from one plane to another given a homography matrix M\n",
    "def warp_point(x, y, M):\n",
    "    d = M[2][0] * x + M[2][1] * y + M[2][2]\n",
    "\n",
    "    return (\n",
    "        int((M[0, 0] * x + M[0, 1] * y + M[0, 2]) / d), # x\n",
    "        int((M[1, 0] * x + M[1, 1] * y + M[1, 2]) / d), # y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4ICFTjb7T20t"
   },
   "outputs": [],
   "source": [
    "#this is like warpPerspectivePadded but for N frames instead of 2\n",
    "def warp_n_with_padding(dst, src_list, transf_list, main_frame_arr):\n",
    "  #main_frame_arr = main_frame_arr2\n",
    "  #dst = main_frame_arr[505]\n",
    "  #src_list = [main_frame_arr[450], main_frame_arr[480], main_frame_arr[508], main_frame_arr[512], main_frame_arr[525]]\n",
    "  #transf_list = [hm13, hm23, hm43, hm53, hm63]\n",
    "\n",
    "  pad_sz_0_arr = []\n",
    "  pad_sz_1_arr = []\n",
    "  minMaxXY_arr = []\n",
    "  dst_sz = list(dst.shape)\n",
    "\n",
    "  for i in range(len(src_list)):\n",
    "    src_h, src_w = src_list[i].shape[:2]\n",
    "    lin_homg_pts = np.array([[0, src_w, src_w, 0], [0, 0, src_h, src_h], [1, 1, 1, 1]])\n",
    "    trans_lin_homg_pts = transf_list[i].dot(lin_homg_pts)\n",
    "    trans_lin_homg_pts /= trans_lin_homg_pts[2,:]\n",
    "\n",
    "    minX = np.min(trans_lin_homg_pts[0,:])\n",
    "    minY = np.min(trans_lin_homg_pts[1,:])\n",
    "    maxX = np.max(trans_lin_homg_pts[0,:])\n",
    "    maxY = np.max(trans_lin_homg_pts[1,:])\n",
    "\n",
    "    pad_sz0 = np.round(np.maximum(dst_sz[0], maxY) - np.minimum(0, minY)).astype(int)\n",
    "    pad_sz1 = np.round(np.maximum(dst_sz[1], maxX) - np.minimum(0, minX)).astype(int)\n",
    "\n",
    "    minMaxXY_arr.append([minX, minY, maxX, maxY])\n",
    "    pad_sz_0_arr.append(pad_sz0)\n",
    "    pad_sz_1_arr.append(pad_sz1)\n",
    "\n",
    "  # calculate the needed padding and create a blank image to place dst within\n",
    "  pad_sz = dst_sz.copy() # to get the same number of channels\n",
    "  pad_sz[0] = max(pad_sz_0_arr)\n",
    "  pad_sz[1] = max(pad_sz_1_arr)\n",
    "  indexY = pad_sz_0_arr.index(pad_sz[0])\n",
    "  indexX = pad_sz_1_arr.index(pad_sz[1])\n",
    "  minY = minMaxXY_arr[indexY][1]\n",
    "  maxY = minMaxXY_arr[indexY][3]\n",
    "  minX = minMaxXY_arr[indexX][0]\n",
    "  maxX = minMaxXY_arr[indexX][2]\n",
    "  dst_pad = np.zeros(pad_sz, dtype=np.uint8)\n",
    "\n",
    "  #add translation to ALL transformation matrices to shift to positive values\n",
    "  new_transf_list = []\n",
    "  anchorX_list = []\n",
    "  anchorY_list = []\n",
    "  for i in range(len(transf_list)):\n",
    "    anchorX, anchorY = 0, 0\n",
    "    transl_transf = np.eye(3,3)\n",
    "    if minX < 0: \n",
    "        anchorX = np.round(-minX).astype(int)\n",
    "        transl_transf[0,2] += anchorX\n",
    "    if minY < 0:\n",
    "        anchorY = np.round(-minY).astype(int)\n",
    "        transl_transf[1,2] += anchorY\n",
    "    new_transf = transl_transf.dot(transf_list[i])\n",
    "    new_transf /= new_transf[2,2]\n",
    "    new_transf_list.append(new_transf)\n",
    "    anchorX_list.append(anchorX)\n",
    "    anchorY_list.append(anchorY)\n",
    "\n",
    "  anchorX = max(anchorX_list)\n",
    "  anchorY = max(anchorY_list)\n",
    "  dst_pad[anchorY:anchorY+dst_sz[0], anchorX:anchorX+dst_sz[1]] = dst\n",
    "\n",
    "  warped_src_arr = []\n",
    "  for i in range(len(src_list)):\n",
    "    warped = cv2.warpPerspective(src_list[i], new_transf_list[i], (pad_sz[1],pad_sz[0]), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "    warped_src_arr.append(warped)\n",
    "  \n",
    "  return dst_pad, warped_src_arr, new_transf_list, anchorX, anchorY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "B-zSJ30PT2x7"
   },
   "outputs": [],
   "source": [
    "#converts all the warped + translated pieces of the panorama from RGB to RGBA images (you'll need the alpha channel [which sets opacity] for compositing)\n",
    "def get_rgba_im_arr(dst_par, warped_src_arr):  \n",
    "  im_arr = []\n",
    "  im = Image.fromarray(dst_pad)\n",
    "  im = im.convert(\"RGBA\")\n",
    "  im = np.asarray(im)\n",
    "  im_arr.append(im)\n",
    "  for i in range(len(warped_src_arr)):\n",
    "    im2 = Image.fromarray(warped_src_arr[i])\n",
    "    im2 = im2.convert(\"RGBA\")\n",
    "    im2 = np.asarray(im2)\n",
    "    im_arr.append(im2)\n",
    "  \n",
    "  return im_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ic-lcmbBUgcH"
   },
   "outputs": [],
   "source": [
    "#converts an array containing an RGB image to an array containing an RGBA image (adds alpha channel)\n",
    "def rgba_to_rgb(comp_arr):\n",
    "  im = Image.fromarray((comp_arr).astype(np.uint8))\n",
    "  im = im.convert('RGB')\n",
    "  im = np.asarray(im)\n",
    "  return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WJ504hliUgZf",
    "outputId": "309b82b2-0d96-44de-e787-3141583bdc5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n #if all images are black, set to black\\n      if len(not_black_list) == 0:\\n        comp_inner.append([0, 0, 0, 255])\\n      '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this function is like. the slowest possible way to do this. Will be updating soon\n",
    "def alpha_composite_n_images(im_arr):\n",
    "  #naive solution\n",
    "  comp = []\n",
    "  im = im_arr[0]\n",
    "  for row in range(im.shape[0]):\n",
    "    comp_inner = []\n",
    "    for col in range(im.shape[1]):\n",
    "      #figure out which images are black at this pixel\n",
    "      not_black_list = []\n",
    "      black_list = []\n",
    "      for i in range(len(im_arr)):\n",
    "        if im_arr[i][row][col][0] == 0 and im_arr[i][row][col][1] == 0 and im_arr[i][row][col][2] == 0:\n",
    "          black_list.append(im_arr[i])\n",
    "        else:\n",
    "          not_black_list.append(im_arr[i])\n",
    "      #if all images are black, set to transparent\n",
    "      if len(not_black_list) == 0:\n",
    "        comp_inner.append([0, 0, 0, 0])\n",
    "      #if only one image is NOT black, use it\n",
    "      elif len(not_black_list) == 1:\n",
    "        comp_inner.append(not_black_list[0][row][col])\n",
    "      #if multiple images are not black, alpha blend them all together\n",
    "      else:\n",
    "        alpha = 1/len(not_black_list)\n",
    "        channel1 = 0\n",
    "        channel2 = 0\n",
    "        channel3 = 0\n",
    "        for j in range(len(not_black_list)):\n",
    "            channel1 = channel1 + alpha * not_black_list[j][row][col][0]\n",
    "            channel2 = channel2 + alpha * not_black_list[j][row][col][1]\n",
    "            channel3 = channel3 + alpha * not_black_list[j][row][col][2]\n",
    "        comp_inner.append([channel1, channel2, channel3, 255])\n",
    "    comp.append(comp_inner)\n",
    "  comp_arr = np.array(comp)\n",
    "  return comp_arr\n",
    "\n",
    "\"\"\"\n",
    " #if all images are black, set to black\n",
    "      if len(not_black_list) == 0:\n",
    "        comp_inner.append([0, 0, 0, 255])\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "yCkWj7FJUoss"
   },
   "outputs": [],
   "source": [
    "def alpha_composite_two(im, im2):\n",
    "  comp = []\n",
    "  #np.zeros((im.shape[0], im.shape[1], im.shape[2]))\n",
    "  alpha = 0.5\n",
    "  for row in range(im.shape[0]):\n",
    "    comp_inner = []\n",
    "    for col in range(im.shape[1]):\n",
    "      #if one image is black, just use the other\n",
    "      if (im[row][col][0] == 0 and im[row][col][1] == 0 and im[row][col][2] == 0) and (im2[row][col][0] != 0 or im2[row][col][1] != 0 or im2[row][col][2] != 0):\n",
    "        comp_inner.append(im2[row][col])\n",
    "      elif (im2[row][col][0] == 0 and im2[row][col][1] == 0 and im2[row][col][2] == 0) and (im[row][col][0] != 0 or im[row][col][1] != 0 or im[row][col][2] != 0):\n",
    "        comp_inner.append(im[row][col])\n",
    "      #if both images are black, set to transparent\n",
    "      elif (im[row][col][0] == 0 and im[row][col][1] == 0 and im[row][col][2] == 0) and (im2[row][col][0] == 0 and im2[row][col][1] == 0 and im2[row][col][2] == 0):\n",
    "        comp_inner.append([0, 0, 0, 0])\n",
    "      #if both pixels are not black, alpha blend\n",
    "      else:\n",
    "        channel1 = alpha * im[row][col][0] + (1 - alpha) * im2[row][col][0]\n",
    "        channel2 = alpha * im[row][col][1] + (1 - alpha) * im2[row][col][1]\n",
    "        channel3 = alpha * im[row][col][2] + (1 - alpha) * im2[row][col][2]\n",
    "        comp_inner.append([channel1, channel2, channel3, 255])\n",
    "    comp.append(comp_inner)\n",
    "  return comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5f-4piGjkmRC"
   },
   "outputs": [],
   "source": [
    "#warps panorama back to rectangle after compositing. Doesn't work for every case yet\n",
    "#this version works by tracing where the corners of the original frames are warped to in the panorama and using those to \"pull\" it back into a rectangle\n",
    "def warp_back_to_rect_up(og_src, org_dst, final_width, final_height, anchorX, anchorY, hm_og_src_og_dst, comp_arr):\n",
    "  og_dst_width = org_dst.shape[1]\n",
    "  og_dst_height =org_dst.shape[0]\n",
    "  og_dst_corners = [[anchorX, anchorY], [anchorX + og_dst_width, anchorY], [anchorX + og_dst_width, anchorY + og_dst_height], [anchorX, anchorY + og_dst_height]] #clockwise from top left\n",
    "\n",
    "  og_src_width = og_src.shape[1]\n",
    "  og_src_height = og_src.shape[0]\n",
    "  og_src_warped_top_left = warp_point(0, 0, hm_og_src_og_dst)\n",
    "  og_src_warped_top_right = warp_point(og_src_width, 0, hm_og_src_og_dst)\n",
    "  og_src_warped_bottom_right = warp_point(og_src_width, og_src_height, hm_og_src_og_dst)\n",
    "  og_src_warped_bottom_left = warp_point(0, og_src_height, hm_og_src_og_dst)\n",
    "  og_src_warped_corners = [og_src_warped_top_left, og_src_warped_top_right, og_src_warped_bottom_right, og_src_warped_bottom_left]\n",
    "\n",
    "  src_quad_list = np.float32([og_dst_corners[0], og_dst_corners[1], og_src_warped_corners[2], og_src_warped_corners[3]])\n",
    "  dst_quad_list = np.float32([[0, 0], [final_width, 0], [final_width, final_height], [0, final_height]])\n",
    "\n",
    "  homography_matrix = cv2.getPerspectiveTransform(src_quad_list, dst_quad_list)\n",
    "\n",
    "  rect = cv2.warpPerspective(comp_arr, homography_matrix, (760, 428))\n",
    "\n",
    "  for j in range(rect.shape[0] - 1):\n",
    "    for i in range(len(rect[0])):\n",
    "      if rect[j][i][3] == 255:\n",
    "        top_left_x = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[0] - 1):\n",
    "    for i in range(len(rect[0])):\n",
    "      if rect[j][(len(rect[0]) - 1) - i][3] == 255:\n",
    "        top_right_x = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[0] - 1):\n",
    "    for i in range(rect.shape[1] - 1):\n",
    "      if rect[(rect.shape[0] - 1) - j][i][3] == 255:\n",
    "        bottom_left_x = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[0] - 1):\n",
    "    for i in range(len(rect[rect.shape[0] - 1])):\n",
    "      if rect[(rect.shape[0] - 1) - j][(rect.shape[0] - 1) - i][3] == 255:\n",
    "        bottom_right_x = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[1] - 1):\n",
    "    for i in range(len(rect[:,0])):\n",
    "      if rect[i][j][3] == 255:\n",
    "        top_left_y = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[1] - 1):\n",
    "    for i in range(rect.shape[0]):\n",
    "      if rect[i][(rect.shape[1] - 1) - j][3] == 255:\n",
    "        top_right_y = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[1] - 1):\n",
    "    for i in range(rect.shape[0]):\n",
    "      if rect[(rect.shape[0] - 1) - i][(rect.shape[1] - 1) - j][3] == 255:\n",
    "        bottom_right_y = i\n",
    "        break\n",
    "\n",
    "  for j in range(rect.shape[1] - 1):\n",
    "    for i in range(len(rect[:,0])):\n",
    "      if rect[(rect.shape[0] - 1) - i][j][3] == 255:\n",
    "        bottom_left_y = i\n",
    "        break\n",
    "\n",
    "  #crop rectangle using where src_quad_list warped to as 4 corners \n",
    "  top_bound = max(top_left_y, top_right_y)\n",
    "  bottom_bound = rect.shape[0] - min(bottom_left_y, bottom_right_y)\n",
    "  left_bound = max(top_left_x, bottom_left_x)\n",
    "  right_bound = rect.shape[1] - min(top_right_x, bottom_right_x)\n",
    "\n",
    "  del_row_arr = []\n",
    "  #rows/y\n",
    "  for i in range(rect.shape[0]):\n",
    "    if i < top_bound or i > bottom_bound:\n",
    "      del_row_arr.append(i)\n",
    "  rect = np.delete(rect, del_row_arr, 0)\n",
    "\n",
    "  del_col_arr = []\n",
    "  #cols/x\n",
    "  for j in range(rect.shape[1]):\n",
    "    if j < left_bound or j > right_bound:\n",
    "      del_col_arr.append(j)\n",
    "  rect = np.delete(rect, del_col_arr, 1)\n",
    "\n",
    "  rect = cv2.resize(rect, (760, 428))\n",
    "\n",
    "  return rect, homography_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "nFo5GIzdoSAu"
   },
   "outputs": [],
   "source": [
    "#crops any fully transparent rows or columns off an image\n",
    "def crop_transparent(comp_arr):\n",
    "  col_sums = np.sum(comp_arr, axis = 0)\n",
    "  del_cols_list = np.where(col_sums == 0)[0]\n",
    "  comp_arr = np.delete(comp_arr, del_cols_list, 1)\n",
    "  row_sums = np.sum(comp_arr, axis = 1)\n",
    "  del_rows_list = np.where(row_sums == 0)[0]\n",
    "  comp_arr = np.delete(comp_arr, del_rows_list, 0)\n",
    "  return comp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "frpbJzYOQZ1e"
   },
   "outputs": [],
   "source": [
    "#this is a helper function used for matching the video frames to object detection outputs\n",
    "# find element closest to given target using binary search.\n",
    " \n",
    "def findClosest(arr, n, target):\n",
    " \n",
    "    # Corner cases\n",
    "    if (target <= arr[0]):\n",
    "        return arr[0]\n",
    "    if (target >= arr[n - 1]):\n",
    "        return arr[n - 1]\n",
    " \n",
    "    # Doing binary search\n",
    "    i = 0; j = n; mid = 0\n",
    "    while (i < j):\n",
    "        mid = (i + j) // 2\n",
    " \n",
    "        if (arr[mid] == target):\n",
    "            return arr[mid]\n",
    " \n",
    "        # If target is less than array\n",
    "        # element, then search in left\n",
    "        if (target < arr[mid]) :\n",
    " \n",
    "            # If target is greater than previous\n",
    "            # to mid, return closest of two\n",
    "            if (mid > 0 and target > arr[mid - 1]):\n",
    "                return getClosest(arr[mid - 1], arr[mid], target)\n",
    " \n",
    "            # Repeat for left half\n",
    "            j = mid\n",
    "         \n",
    "        # If target is greater than mid\n",
    "        else :\n",
    "            if (mid < n - 1 and target < arr[mid + 1]):\n",
    "                return getClosest(arr[mid], arr[mid + 1], target)\n",
    "                 \n",
    "            # update i\n",
    "            i = mid + 1\n",
    "         \n",
    "    # Only single element left after search\n",
    "    return arr[mid]\n",
    " \n",
    " \n",
    "# Method to compare which one is the more close.\n",
    "# We find the closest by taking the difference\n",
    "# between the target and both values. It assumes\n",
    "# that val2 is greater than val1 and target lies\n",
    "# between these two.\n",
    "def getClosest(val1, val2, target):\n",
    " \n",
    "    if (target - val1 >= val2 - target):\n",
    "        return val2\n",
    "    else:\n",
    "        return val1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nHUTOaV9Qfs_"
   },
   "outputs": [],
   "source": [
    "def get_sec_from_start(current_timestamp, start_timestamp):\n",
    "  sec_from_start = (float(current_timestamp[0:-2]) - float(start_timestamp[0:-2]))/1000\n",
    "  return sec_from_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OBkBSEuh75D1"
   },
   "outputs": [],
   "source": [
    "def warp_one_bbox(x, y, w, h, M):\n",
    "  \n",
    "  #warp top left corner \n",
    "  new_x, new_y = warp_point(x, y, M)\n",
    "\n",
    "  #warp bottom right corner\n",
    "  new_w, new_h = warp_point(w, h, M)\n",
    "\n",
    "  return new_x, new_y, new_w, new_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ROdNi9aN-oG1"
   },
   "outputs": [],
   "source": [
    "#gets the x,y coords of how much the destination image was translated during the warping process\n",
    "def get_anchors(dst, src_list, transf_list, main_frame_arr):\n",
    "  pad_sz_0_arr = []\n",
    "  pad_sz_1_arr = []\n",
    "  minMaxXY_arr = []\n",
    "  dst_sz = list(dst.shape)\n",
    "\n",
    "  for i in range(len(src_list)):\n",
    "    src_h, src_w = src_list[i].shape[:2]\n",
    "    lin_homg_pts = np.array([[0, src_w, src_w, 0], [0, 0, src_h, src_h], [1, 1, 1, 1]])\n",
    "    trans_lin_homg_pts = transf_list[i].dot(lin_homg_pts)\n",
    "    trans_lin_homg_pts /= trans_lin_homg_pts[2,:]\n",
    "\n",
    "    minX = np.min(trans_lin_homg_pts[0,:])\n",
    "    minY = np.min(trans_lin_homg_pts[1,:])\n",
    "    maxX = np.max(trans_lin_homg_pts[0,:])\n",
    "    maxY = np.max(trans_lin_homg_pts[1,:])\n",
    "\n",
    "    pad_sz0 = np.round(np.maximum(dst_sz[0], maxY) - np.minimum(0, minY)).astype(int)\n",
    "    pad_sz1 = np.round(np.maximum(dst_sz[1], maxX) - np.minimum(0, minX)).astype(int)\n",
    "\n",
    "    minMaxXY_arr.append([minX, minY, maxX, maxY])\n",
    "    pad_sz_0_arr.append(pad_sz0)\n",
    "    pad_sz_1_arr.append(pad_sz1)\n",
    "\n",
    "  # calculate the needed padding and create a blank image to place dst within\n",
    "  pad_sz = dst_sz.copy() # to get the same number of channels\n",
    "  pad_sz[0] = max(pad_sz_0_arr)\n",
    "  pad_sz[1] = max(pad_sz_1_arr)\n",
    "  indexY = pad_sz_0_arr.index(pad_sz[0])\n",
    "  indexX = pad_sz_1_arr.index(pad_sz[1])\n",
    "  minY = minMaxXY_arr[indexY][1]\n",
    "  maxY = minMaxXY_arr[indexY][3]\n",
    "  minX = minMaxXY_arr[indexX][0]\n",
    "  maxX = minMaxXY_arr[indexX][2]\n",
    "  #dst_pad = np.zeros(pad_sz, dtype=np.uint8)\n",
    "\n",
    "  #add translation to ALL transformation matrices to shift to positive values\n",
    "  new_transf_list = []\n",
    "  anchorX_list = []\n",
    "  anchorY_list = []\n",
    "  for i in range(len(transf_list)):\n",
    "    anchorX, anchorY = 0, 0\n",
    "    transl_transf = np.eye(3,3)\n",
    "    if minX < 0: \n",
    "        anchorX = np.round(-minX).astype(int)\n",
    "        transl_transf[0,2] += anchorX\n",
    "    if minY < 0:\n",
    "        anchorY = np.round(-minY).astype(int)\n",
    "        transl_transf[1,2] += anchorY\n",
    "    new_transf = transl_transf.dot(transf_list[i])\n",
    "    new_transf /= new_transf[2,2]\n",
    "    new_transf_list.append(new_transf)\n",
    "    anchorX_list.append(anchorX)\n",
    "    anchorY_list.append(anchorY)\n",
    "\n",
    "  anchorX = max(anchorX_list)\n",
    "  anchorY = max(anchorY_list)\n",
    "  #dst_pad[anchorY:anchorY+dst_sz[0], anchorX:anchorX+dst_sz[1]] = dst\n",
    "\n",
    "  #warped_src_arr = []\n",
    "  #for i in range(len(src_list)):\n",
    "    #warped = cv2.warpPerspective(src_list[i], new_transf_list[i], (pad_sz[1],pad_sz[0]), borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
    "    #warped_src_arr.append(warped)\n",
    "  \n",
    "  return anchorX, anchorY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAazVQNu82E1"
   },
   "source": [
    "END OF FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "CdOt2JFh9oXl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Videos:\n",
      " /Users/korahughes/Documents/GitHub/panoramic-mosaics/data/avis/2023.04.11-17.25.01-main.avi\n",
      " /Users/korahughes/Documents/GitHub/panoramic-mosaics/data/avis/2023.03.20-19.05.44-main.avi\n",
      " /Users/korahughes/Documents/GitHub/panoramic-mosaics/data/avis/2023.03.29-17.39.21-main.avi\n",
      "Found Boxes:\n",
      " /Users/korahughes/Documents/GitHub/panoramic-mosaics/data/object_detection/2023.04.11-17.25.01-detic:image.json\n",
      " /Users/korahughes/Documents/GitHub/panoramic-mosaics/data/object_detection/2023.03.20-19.05.44-detic:image.json\n",
      " /Users/korahughes/Documents/GitHub/panoramic-mosaics/data/object_detection/2023.03.29-17.39.21-detic:image.json\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive',force_remount=True)\n",
    "\n",
    "# vars for jupyter notebook\n",
    "cwd = os.getcwd()  # current local dir\n",
    "input_dir = cwd + \"/data/\"\n",
    "output_dir = cwd + \"/results/\"\n",
    "\n",
    "test_videos = [input_dir+\"avis/\"+file for file in os.listdir(input_dir+\"avis/\") if file[-4:]==\".avi\"]\n",
    "print(\"Found Videos:\\n\", '\\n '.join(test_videos))\n",
    "\n",
    "test_boxes = [input_dir+\"object_detection/\"+file for file in os.listdir(input_dir+\"object_detection/\") if file[-5:]==\".json\"]\n",
    "print(\"Found Boxes:\\n\", '\\n '.join(test_boxes))\n",
    "\n",
    "chunks_to_process = 5  # 85"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IiAcbWvy9ssl"
   },
   "source": [
    "2023.03.29-17.39.21 stats (these should be the same for every video but it's always good to check):\n",
    "\n",
    "*  video is 15 fps\n",
    "*  bboxes are 1.69 fps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9NHGeg-89Vj"
   },
   "source": [
    "CODE TO GENERATE PANORAMAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "qRHEKBlv9pD1"
   },
   "outputs": [],
   "source": [
    "#I don't know why but openCV gets mad if the video is in any format but AVI. I use this to convert mp4 to avi: https://cloudconvert.com/mp4-to-avi\n",
    "main_frame_arr = video_to_frame_arr(test_videos[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "SOaIyS0G9utN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunk 0...\n",
      "Not enough matches are found - 1/4\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/core/src/matmul.dispatch.cpp:550: error: (-215:Assertion failed) scn + 1 == m.cols in function 'perspectiveTransform'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m kp2, des2 \u001b[38;5;241m=\u001b[39m get_keypoints_descriptors(new_frame)\n\u001b[1;32m     45\u001b[0m matches12 \u001b[38;5;241m=\u001b[39m feature_matching(des1, des2)\n\u001b[0;32m---> 46\u001b[0m hm12 \u001b[38;5;241m=\u001b[39m \u001b[43mget_homography_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_comp_arr1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_frame\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkp2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatches12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(hm12) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     49\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mget_homography_matrix\u001b[0;34m(image, image2, kp, kp2, good, MIN_MATCH_COUNT)\u001b[0m\n\u001b[1;32m      8\u001b[0m     w \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      9\u001b[0m     pts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32([ [\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m0\u001b[39m,h\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],[w\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,h\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],[w\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m] ])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     dst \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperspectiveTransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpts\u001b[49m\u001b[43m,\u001b[49m\u001b[43mM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#image2_lines = cv2.polylines(image2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough matches are found - \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(good), MIN_MATCH_COUNT) )\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /Users/runner/work/opencv-python/opencv-python/opencv/modules/core/src/matmul.dispatch.cpp:550: error: (-215:Assertion failed) scn + 1 == m.cols in function 'perspectiveTransform'\n"
     ]
    }
   ],
   "source": [
    "#each time this takes 0, 6, 12, 18, 24, 30, 36, 42, 47th indices from current window\n",
    "#windows are currently discrete chunks of the array (doing one for each timestep was taking too long to process but it's the same idea)\n",
    "main_frame_arr2 = main_frame_arr\n",
    "pano_frames = []\n",
    "start_index = 0\n",
    "start_chunk = 0\n",
    "done = 0\n",
    "#filename_count = 0\n",
    "indices = [12, 18, 24, 30, 36, 42, 47]\n",
    "#these three arrays are just grabbing the homography matrices I was trying to use to warp the bounding boxes they're not needed to make the panorama itself\n",
    "hm12_pre_warp_n_with_padding_arr = []\n",
    "hm12_post_warp_n_with_padding_arr = []\n",
    "hm_rect_arr = []\n",
    "\n",
    "for i in range(chunks_to_process): #this is for this specific example, but it's range(number of chunks you're diving the video into, where each chunk becomes one panorama)\n",
    "  print(\"processing chunk\", str(i)+\"...\")\n",
    "  hm12_pre_warp_n_with_padding_arr_inner = []\n",
    "  hm12_post_warp_n_with_padding_arr_inner = []\n",
    "  hm_rect_arr_inner = [] \n",
    "\n",
    "  start_chunk = 47* i + start_index + (47 * done)\n",
    "  window_frame1 = main_frame_arr2[start_chunk + 0]\n",
    "  window_frame2 = main_frame_arr2[start_chunk + 6]\n",
    "  kp1, des1 =get_keypoints_descriptors(window_frame1)\n",
    "  kp2, des2 = get_keypoints_descriptors(window_frame2)\n",
    "  matches12 = feature_matching(des1, des2)\n",
    "  hm12 = get_homography_matrix(window_frame1, window_frame2, kp1, kp2, matches12, 4)\n",
    "  hm12_pre_warp_n_with_padding_arr_inner.append(hm12)\n",
    "\n",
    "  if len(hm12) == 0:\n",
    "    comp_arr1 = window_frame2\n",
    "  else:\n",
    "    #NEED TO USE THE NEW HM FROM HERE\n",
    "    dst_pad, warped_src_arr, new_transf_list, anchorX, anchorY = warp_n_with_padding(window_frame2, [window_frame1], [hm12], main_frame_arr2)\n",
    "    hm12_post_warp_n_with_padding_arr_inner.append(new_transf_list[0])\n",
    "    im_arr1 = get_rgba_im_arr(dst_pad, warped_src_arr)\n",
    "    comp_arr1 = alpha_composite_n_images(im_arr1)\n",
    "\n",
    "  #ROUNDS 2-6\n",
    "  for j in range(5):\n",
    "    new_frame = main_frame_arr2[start_chunk + indices[j]]\n",
    "    rgb_comp_arr1 = rgba_to_rgb(comp_arr1)\n",
    "    kp1, des1 = get_keypoints_descriptors(rgb_comp_arr1)\n",
    "    kp2, des2 = get_keypoints_descriptors(new_frame)\n",
    "    matches12 = feature_matching(des1, des2)\n",
    "    hm12 = get_homography_matrix(rgb_comp_arr1, new_frame, kp1, kp2, matches12, 4)\n",
    "\n",
    "    if len(hm12) == 0:\n",
    "      continue\n",
    "    else:\n",
    "      dst_pad, warped_src_arr, new_transf_list, anchorX, anchorY = warp_n_with_padding(new_frame, [rgb_comp_arr1], [hm12], main_frame_arr2)\n",
    "      im_arr1 = get_rgba_im_arr(dst_pad, warped_src_arr)\n",
    "      comp_arr1 = alpha_composite_n_images(im_arr1)\n",
    "\n",
    "  cv2.imwrite(\"/content/case_study_pre_rect_panos/\" + str(done) + \".png\", comp_arr1)\n",
    "  rect1, homography_matrix_rect = warp_back_to_rect_up(rgb_comp_arr1, new_frame, 760, 428, anchorX, anchorY, hm12, comp_arr1)\n",
    "  hm_rect_arr_inner.append(homography_matrix_rect)\n",
    "\n",
    "  rect1 = crop_transparent(rect1)\n",
    "  pano_frames.append(rect1)\n",
    "  cv2.imwrite(\"/content/case_study_panos_manual/\" + str(done) + \".png\", rect1)\n",
    "  #filename_count = filename_count + 1\n",
    "  print(start_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture(test_videos[2])\n",
    "fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(f\"{fps} frames per second\")\n",
    "\n",
    "with open(test_boxes[2], 'r') as j:\n",
    "     detic_dict = json.loads(j.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the # of seconds from start for each frame\n",
    "frame_sfs_arr = []\n",
    "sfs = 0\n",
    "for i in range(len(main_frame_arr)):\n",
    "  frame_sfs_arr.append(sfs)\n",
    "  sfs = sfs + (1/15)\n",
    "    \n",
    "#have to match frames with timestamps (each frame needs to have a timestamp)\n",
    "timestamps_sfs_arr = []\n",
    "start_timestamp = detic_dict[0][\"timestamp\"]\n",
    "for i in range(len(detic_dict)):\n",
    "  current_timestamp = detic_dict[i][\"timestamp\"]\n",
    "  sec_from_start = get_sec_from_start(current_timestamp, start_timestamp)\n",
    "  timestamps_sfs_arr.append(sec_from_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames_timestamps_arr = []  # TODO: find out what this is doing\n",
    "arr = timestamps_sfs_arr\n",
    "n = len(arr)\n",
    "for i in range(len(main_frame_arr)):\n",
    "  target = frame_sfs_arr[i]\n",
    "  closest = findClosest(arr, n, target)\n",
    "  target_idx = timestamps_sfs_arr.index(closest)\n",
    "  timestamp = detic_dict[target_idx][\"timestamp\"]\n",
    "  frames_timestamps_arr.append(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Bounding Box / time\n",
    "- at every time step, draw all bounding boxes from the beginning of the vis to now\n",
    "- rgba : a inversely proportional to (distance from the present)^2\n",
    "-- play around with color, thickness & how much of the box is being drawn (whole box vs. picture-frame vs. just corners)\n",
    "\"\"\"\n",
    "# time represented by blue-red color scale: stolen from d3.interpolateOrRd - https://observablehq.com/@d3/color-schemes\n",
    "OrRd = [\"#fff7ec\",\"#fff7eb\",\"#fff6ea\",\"#fff6e9\",\"#fff5e7\",\"#fff5e6\",\"#fff4e5\",\"#fff4e4\",\"#fff3e3\",\"#fff3e2\",\"#fff2e1\",\"#fff2e0\",\"#fff1de\",\"#fff1dd\",\"#fff0dc\",\"#fff0db\",\"#feefda\",\"#feefd9\",\"#feeed7\",\"#feeed6\",\"#feedd5\",\"#feedd4\",\"#feecd3\",\"#feecd2\",\"#feebd0\",\"#feebcf\",\"#feeace\",\"#feeacd\",\"#fee9cc\",\"#fee9ca\",\"#fee8c9\",\"#fee8c8\",\"#fee7c7\",\"#fee7c6\",\"#fee6c4\",\"#fee5c3\",\"#fee5c2\",\"#fee4c1\",\"#fee4bf\",\"#fee3be\",\"#fee3bd\",\"#fee2bc\",\"#fee1ba\",\"#fee1b9\",\"#fee0b8\",\"#fee0b7\",\"#fedfb5\",\"#fedeb4\",\"#fedeb3\",\"#fdddb2\",\"#fddcb1\",\"#fddcaf\",\"#fddbae\",\"#fddaad\",\"#fddaac\",\"#fdd9ab\",\"#fdd8a9\",\"#fdd8a8\",\"#fdd7a7\",\"#fdd6a6\",\"#fdd6a5\",\"#fdd5a4\",\"#fdd4a3\",\"#fdd4a1\",\"#fdd3a0\",\"#fdd29f\",\"#fdd29e\",\"#fdd19d\",\"#fdd09c\",\"#fdcf9b\",\"#fdcf9a\",\"#fdce99\",\"#fdcd98\",\"#fdcc97\",\"#fdcc96\",\"#fdcb95\",\"#fdca94\",\"#fdc994\",\"#fdc893\",\"#fdc892\",\"#fdc791\",\"#fdc690\",\"#fdc58f\",\"#fdc48e\",\"#fdc38d\",\"#fdc28c\",\"#fdc18b\",\"#fdc08a\",\"#fdbf89\",\"#fdbe88\",\"#fdbd87\",\"#fdbc86\",\"#fdbb85\",\"#fdba84\",\"#fdb983\",\"#fdb882\",\"#fdb781\",\"#fdb680\",\"#fdb57f\",\"#fdb47d\",\"#fdb27c\",\"#fdb17b\",\"#fdb07a\",\"#fdaf79\",\"#fdae78\",\"#fdac76\",\"#fdab75\",\"#fdaa74\",\"#fca873\",\"#fca772\",\"#fca671\",\"#fca46f\",\"#fca36e\",\"#fca26d\",\"#fca06c\",\"#fc9f6b\",\"#fc9e6a\",\"#fc9c68\",\"#fc9b67\",\"#fb9a66\",\"#fb9865\",\"#fb9764\",\"#fb9563\",\"#fb9462\",\"#fb9361\",\"#fb9160\",\"#fa905f\",\"#fa8f5e\",\"#fa8d5d\",\"#fa8c5c\",\"#f98b5b\",\"#f9895a\",\"#f98859\",\"#f98759\",\"#f88558\",\"#f88457\",\"#f88356\",\"#f78155\",\"#f78055\",\"#f77f54\",\"#f67d53\",\"#f67c52\",\"#f67b52\",\"#f57951\",\"#f57850\",\"#f4774f\",\"#f4754f\",\"#f4744e\",\"#f3734d\",\"#f3714c\",\"#f2704c\",\"#f26f4b\",\"#f16d4a\",\"#f16c49\",\"#f06b49\",\"#f06948\",\"#ef6847\",\"#ef6646\",\"#ee6545\",\"#ed6344\",\"#ed6243\",\"#ec6042\",\"#ec5f42\",\"#eb5d41\",\"#ea5c40\",\"#ea5a3f\",\"#e9593e\",\"#e8573c\",\"#e8563b\",\"#e7543a\",\"#e65339\",\"#e65138\",\"#e55037\",\"#e44e36\",\"#e44c35\",\"#e34b34\",\"#e24932\",\"#e14831\",\"#e04630\",\"#e0442f\",\"#df432e\",\"#de412d\",\"#dd402b\",\"#dc3e2a\",\"#dc3c29\",\"#db3b28\",\"#da3927\",\"#d93826\",\"#d83624\",\"#d73423\",\"#d63322\",\"#d53121\",\"#d43020\",\"#d32e1f\",\"#d22c1e\",\"#d12b1d\",\"#d0291b\",\"#cf281a\",\"#ce2619\",\"#cd2518\",\"#cc2317\",\"#cb2216\",\"#ca2015\",\"#c91f14\",\"#c81d13\",\"#c71c12\",\"#c61b11\",\"#c51911\",\"#c31810\",\"#c2170f\",\"#c1150e\",\"#c0140d\",\"#bf130c\",\"#be120c\",\"#bc110b\",\"#bb100a\",\"#ba0e09\",\"#b80d09\",\"#b70c08\",\"#b60b07\",\"#b50b07\",\"#b30a06\",\"#b20906\",\"#b10805\",\"#af0705\",\"#ae0704\",\"#ac0604\",\"#ab0504\",\"#a90503\",\"#a80403\",\"#a60402\",\"#a50302\",\"#a40302\",\"#a20302\",\"#a00201\",\"#9f0201\",\"#9d0201\",\"#9c0101\",\"#9a0101\",\"#990101\",\"#970101\",\"#960100\",\"#940100\",\"#920000\",\"#910000\",\"#8f0000\",\"#8e0000\",\"#8c0000\",\"#8a0000\",\"#890000\",\"#870000\",\"#860000\",\"#840000\",\"#820000\",\"#810000\",\"#7f0000\"]\n",
    "justRed = [\"#ff0000\"]  # typical bright red bounding box\n",
    "time_color = OrRd  # Note: we want the last ind to be the color of the most recent frame & the 0th ind to be the furthest\n",
    "\n",
    "def box_color(time_from_present, max_time=5):  # assumed seconds\n",
    "    \"\"\" \n",
    "    input: box's time from the present moment, how far in the past boxes should show up\n",
    "    output: [r, g, b, a] value of the box [0,255]\n",
    "    \"\"\"\n",
    "    if time_from_present >= max_time:\n",
    "        return [0,0,0,0]\n",
    "    color_factor = (1 - time_from_present/max_time)**2  # get the squared dist from max time by normalizing [0,1]\n",
    "    hex_color = time_color[round(color_factor * (len(time_color)-1))]\n",
    "    return hex_to_rgb(hex_color) + [round(color_factor*255)]  # add alpha scaler to return\n",
    "\n",
    "def hex_to_rgb(hex_str):\n",
    "    \"\"\" helper to convert rgb hex to its individual values in a list \n",
    "    ref: https://stackoverflow.com/questions/29643352/converting-hex-to-rgb-value-in-python \"\"\"\n",
    "    h = hex_str.lstrip('#')\n",
    "    return list(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "\n",
    "\n",
    "print(\"Example color conversion:\\n\", time_color[0], \"=>\", hex_to_rgb(time_color[0]))\n",
    "print(\"Example Bounding Box colors (max_time=5, index=sec_passed):\\n\", [box_color(i) for i in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Reasources:\n",
    "- highlighting an area - https://stackoverflow.com/questions/56472024/how-to-change-the-opacity-of-boxes-cv2-rectangle\n",
    "- cv2 draw functions - https://docs.opencv.org/4.x/dc/da5/tutorial_py_drawing_functions.html\n",
    "\"\"\"\n",
    "\n",
    "rect_size = 2  # size of boxes that denote corners\n",
    "def draw_box_corners(image, x,y, w,h, color, thickness):\n",
    "    # TODO: figure out if x,y is top left or bottom left corner\n",
    "    image = cv2.rectangle(image, (x,y), (rect_size, rect_size), color, thickness)\n",
    "    image = cv2.rectangle(image, (x+w,y), (rect_size, rect_size), color, thickness)\n",
    "    image = cv2.rectangle(image, (x,y-h), (rect_size, rect_size), color, thickness)\n",
    "    return cv2.rectangle(image, (x+w,y-h), (rect_size, rect_size), color, thickness)\n",
    "\n",
    "\n",
    "line_len = 4;\n",
    "def draw_box_frame(image, x,y, w,h, color, thickness):\n",
    "    # TODO: tweak positioning\n",
    "    image = cv2.line(image, (x,y), (x+line_len, y), color, thickness)\n",
    "    image = cv2.line(image, (x,y), (x, y-line_len), color, thickness)\n",
    "    \n",
    "    image = cv2.line(image, (x+w-line_len,y), (x+w,y), color, thickness)\n",
    "    image = cv2.line(image, (x+w,y), (x+w,y-line_len), color, thickness)\n",
    "    \n",
    "    image = cv2.line(image, (x,y-h), (x+line_len, y-h), color, thickness)\n",
    "    image = cv2.line(image, (x,y-h+line_len), (x, y-h), color, thickness)\n",
    "    \n",
    "    image = cv2.line(image, (x+w-line_len,y-h), (x+w, y-h), color, thickness)\n",
    "    return cv2.line(image, (x+w,y-h+line_len), (x+w, y-h), color, thickness)\n",
    "    \n",
    "    \n",
    "def draw_box(image, x,y, w,h, color, thickness):\n",
    "    return cv2.rectangle(image, (x,y), (w,h), color, thickness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = next((i for i, obj in enumerate(detic_dict) if obj['timestamp'] == frames_timestamps_arr[500]), -1)\n",
    "\n",
    "#actually draw boxes on\n",
    "for j in range(len(main_frame_arr)):  # for each frame\n",
    "  color = (0, 0, 255)\n",
    "  thickness = 2\n",
    "    \n",
    "#   for time_since_present in range(1, j+1):  # draw boxes for past images\n",
    "#     image = draw_box(image, x, y, w, h, box_color(time_since_present), thickness)\n",
    "    \n",
    "  # draw boxes for present\n",
    "  image = main_frame_arr[j]\n",
    "  index = next((i for i, obj in enumerate(detic_dict) if obj['timestamp'] == frames_timestamps_arr[j]), -1)\n",
    "  for i in range(len(detic_dict[index][\"values\"])):  # for each detected object?\n",
    "    x = int(detic_dict[index][\"values\"][i][\"xyxyn\"][0] * 760)\n",
    "    w = int(detic_dict[index][\"values\"][i][\"xyxyn\"][2] * 760)\n",
    "    y = int(detic_dict[index][\"values\"][i][\"xyxyn\"][1] * 428)\n",
    "    h = int(detic_dict[index][\"values\"][i][\"xyxyn\"][3] * 428)\n",
    "    start_point = (x, y)\n",
    "    end_point = (w, h)\n",
    "    # draw object bounding box\n",
    "    image = cv2.rectangle(image, start_point, end_point, color, thickness)\n",
    "#     image = draw_box(image, x, y, w, h, box_color(j), thickness)\n",
    "\n",
    "    # Draw red background rectangle\n",
    "    image = cv2.rectangle(image, (x, y-15), (x + (w - x), y), (0,0,255), -1)  # TODO: figure this out\n",
    "\n",
    "    # Add text\n",
    "    image = cv2.putText(image, detic_dict[index][\"values\"][i][\"label\"], (x + 2,y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255,255,255), 1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
